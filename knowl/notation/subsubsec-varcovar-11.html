<!DOCTYPE html>
<html lang="fr-FR" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<article class="exploration project-like"><h6 class="heading">
<span class="type">Exploration</span><span class="codenumber"> 1.17 - </span> <span class="title">Matrice des covariances d’un vecteur aléatoire.</span>
</h6>
<div class="para">On considère des VADR <span class="process-math">\(X_1,X_2,\ldots,X_n\text{.}\)</span> On appelle matrice des covariances du vecteur <span class="process-math">\(X=(X_1,X_2,\ldots,X_n)\)</span> la matrice carrée <span class="process-math">\(\Sigma(X)\)</span> de taille <span class="process-math">\(n\times n\)</span> dont les coefficients sont <span class="process-math">\(\Cov(X_i,X_j)\text{.}\)</span> <span class="process-math">\(\Sigma(X)\)</span> est la matrice de Gram de la famille <span class="process-math">\((X_1,X_2,\ldots,X_n)\)</span> pour la forme bilinéaire symétrique <span class="process-math">\(\Cov\text{.}\)</span>
</div>
<div class="para logical">
<div class="para">Pour tout <span class="process-math">\((\lambda_1,\lambda_2,\ldots,\lambda_n)\in\R^n\text{,}\)</span> on a</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/task-sympos.html">
\begin{equation*}
\VV(\lambda_1X_1+\lambda_2X_2+\cdots+\lambda_nX_n)={}^t\!\Lambda \Sigma(X)\Lambda
\end{equation*}
</div>
<div class="para">où <span class="process-math">\(\Lambda={}^t\!\begin{pmatrix}\lambda_1\amp \lambda_2\amp \cdots\amp \lambda_n\end{pmatrix}\)</span> (voir l’activité <a href="sec-espvaralea.html#task-sympos" class="xref" data-knowl="./knowl/xref/task-sympos.html" data-reveal-label="Dévoiler" data-close-label="Fermer" title="Question 1.18.a: Symétrie et positivité">Question 1.18.a</a>). On en déduit par exemple que <span class="process-math">\(\lambda_1X_1+\lambda_2X_2+\cdots+\lambda_nX_n\)</span> est presque partout constante  si et seulement si <span class="process-math">\(\Lambda\in\ker\Sigma(X)\text{.}\)</span>
</div>
</div>
<div class="para logical">
<div class="para">
<span class="process-math">\(\Sigma(X)\)</span> est une matrice symétrique positive. Elle est en particulier orthogonalemnt diagonalisable et ses valeurs propres sont positives ou nulles. Une base orthonormale de vecteurs propres peut servir à construire une base de l’espace vectoriel engendré par <span class="process-math">\(X_1,X_2,\ldots,X_n\)</span> formée de VADR <em class="emphasis">deux à deux non corrélées</em>. En outre les vecteurs dans cette base qui sont associés à la valeur propre nulle sont des VADR presque partout constantes. Ce qui permet de représenter tout élément de <span class="process-math">\(\mathrm{Vect}\{X_1,X_2,\ldots,X_n\}\)</span> à une VADR presque partout constante près comme une combinaison linéaire de <span class="process-math">\(r=\operatorname{rg}\Sigma(X)\)</span> VADR non partout constantes et deux à deux non corrélées. Ces observations sont essentielles dans ce qu’on appelle Analyse en Composantes Principales (ACP). Voir pour cela :</div>
<ul class="disc" id="subsubsec-varcovar-11-5-6">
<li><div class="para">la définition sur <a class="external" href="https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales" target="_blank">Wikipedia</a><details class="ptx-footnote" aria-live="polite" id="subsubsec-varcovar-11-5-6-1-1-2"><summary class="ptx-footnote__number" title="Note de bas de page 1.5"><sup> 5 </sup></summary><div class="ptx-footnote__contents" id="subsubsec-varcovar-11-5-6-1-1-2"><code class="code-inline tex2jax_ignore">fr.wikipedia.org/wiki/Analyse_en_composantes_principales</code></div></details>
</div></li>
<li><div class="para">un document plus technique est plus détaillé sur <a class="external" href="http://www.mderouich.ma/web/Enseignements%202020/chapitreIII.pdf" target="_blank">le web</a><details class="ptx-footnote" aria-live="polite" id="subsubsec-varcovar-11-5-6-2-1-2"><summary class="ptx-footnote__number" title="Note de bas de page 1.6"><sup> 6 </sup></summary><div class="ptx-footnote__contents" id="subsubsec-varcovar-11-5-6-2-1-2"><code class="code-inline tex2jax_ignore">www.mderouich.ma/web/Enseignements%202020/chapitreIII.pdf</code></div></details>
</div></li>
</ul>
<div class="para">D’autre propriétés de la matrices de covariances seront en outre explorées dans les activités <a href="sec-espvaralea.html#matrice-covariance-vecteur-aleatoire" class="xref" data-knowl="./knowl/xref/matrice-covariance-vecteur-aleatoire.html" data-reveal-label="Dévoiler" data-close-label="Fermer" title="Activité 1.18: Matrice des covariances d’un vecteur aléatoire">Activité 1.18</a> et <a href="sec-espvaralea.html#esperance-forme-quadratique" class="xref" data-knowl="./knowl/xref/esperance-forme-quadratique.html" data-reveal-label="Dévoiler" data-close-label="Fermer" title="Activité 1.19: Espérance d’une forme quadratique">Activité 1.19</a>
</div>
</div></article><span class="incontext"><a class="internal" href="sec-espvaralea.html#subsubsec-varcovar-11">Contexte</a></span>
</body>
</html>
