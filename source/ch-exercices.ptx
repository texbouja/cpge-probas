<?xml version="1.0" encoding="UTF-8"?>





<chapter xml:id="ch-exercices">
    <title>Exercices</title>
    



<exercise><title>Caractérisation de la loi géometrique décalée</title>


    <introduction>
    <p>
        On considère <m> X </m> et <m> Y </m> deux variables aléatoires sur <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>, à valeurs dans <m> \mathbb{N} </m>, indépendantes, de même loi. On pose <m> D = X - Y </m> et <m> I = \min(X, Y) </m>.
    </p>
    </introduction>

    <task>
        <statement>
            On suppose que pour tout <m> k </m> dans <m> \mathbb{N} </m>, <m> \mathbb{P}(X = k) = pq^k </m>, où <m> p \in [0, 1[ </m> et <m> q = 1 - p </m>.
            <ol>
                <li>Déterminer la loi conjointe de <m> (D, I) </m>.</li>
                <li>Déterminer les lois marginales de <m> D </m> et <m> I </m>. Vérifier que <m> D </m> et <m> I </m> sont indépendantes.</li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    <p>
                    On a <m> D(\Omega) = \mathbb{Z} </m> et <m> I(\Omega) = \mathbb{N} </m>.
                    <ul>
                        <li><p>Si <m> k \geq 0 </m>, alors on a :
                            <me>
                            \{D = k\} \cap \{I = \ell\} = \{X - Y = k\} \cap \{Y = \ell\} = \{X = k + \ell\} \cap \{Y = \ell\}
                            </me>
                            et donc, par indépendance de <m> X </m> et <m> Y </m> :
                            <me>
                            \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(\{X = k + \ell\} \cap \{Y = \ell\}) = pq^{k + \ell} pq^\ell = p^2 q^{k + 2\ell}.
                            </me>
                            </p>
                        </li>
                        <li>
                            <p> Si <m> k \lt 0 </m>, alors on a :
                            <me>
                            \{D = k\} \cap \{I = \ell\} = \{X - Y = k\} \cap \{X = \ell\} = \{X = \ell\} \cap \{Y = -k + \ell\}
                            </me>
                            et donc, par indépendance de <m> X </m> et <m> Y </m> :
                            <me>
                            \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(\{X = \ell\} \cap \{Y = -k + \ell\}) = pq^\ell pq^{-k + \ell} = p^2 q^{-k + 2\ell}.
                            </me>
                            </p>
                        </li>
                    </ul>
                    </p>
                    <p>
                    Dans tous les cas, on trouve <m> \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = p^2 q^{|k| + 2\ell} </m>.
                    </p>
                </li>
                <li>
                    <ul>
                        <li>Pour <m> k \in \mathbb{Z} </m>, on a :
                            <me>
                            \mathbb{P}(D = k) = \sum_{\ell=0}^{+\infty} \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \sum_{\ell=0}^{+\infty} p^2 q^{|k| + 2\ell} = \frac{p^2 q^{|k|}}{1 - q^2} = \frac{p q^{|k|}}{1 + q}.
                            </me>
                        </li>
                        <li>Pour <m> \ell \in \mathbb{N} </m>, on a :
                            <me>
                            \mathbb{P}(I = \ell) = \sum_{k=-\infty}^{+\infty} \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \sum_{k=-\infty}^{+\infty} p^2 q^{|k| + 2\ell} = p^2 q^{2\ell} \left( \frac{2}{1 - q} - 1 \right) = p q^{2\ell} (1 + q).
                            </me>
                        </li>
                    </ul>
                    <p>
                    On vérifie que <m> D </m> et <m> I </m> sont indépendantes car :
                    <me>
                    \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(D = k) \mathbb{P}(I = \ell).
                    </me>
                    </p>
                </li>
            </ol>
        </solution>
    </task>

    <task>
        <statement>
            <p>
            On suppose que les variables <m> D </m> et <m> I </m> sont indépendantes et que <m> \mathbb{P}(X = n) \neq 0 </m> pour tout <m> n \in \mathbb{N} </m>. Montrer qu'il existe <m> p \in [0, 1[ </m>, tel que, pour tout <m> k \in \mathbb{N} </m>,
            <me>
            \mathbb{P}(X = k) = p q^k.
            </me>
            </p>
        </statement>
        <solution>
            <p>
            Comme précédemment, on a :
            <me>
            \forall k \in \mathbb{N}, \quad \{D = k\} \cap \{I = \ell\} = \{X = k + \ell\} \cap \{Y = \ell\}.
            </me>
            Par indépendance de <m> X </m> et <m> Y </m> d'une part, de <m> D </m> et <m> I </m> d'autre part, on en déduit :
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = \ell) = \mathbb{P}(X = k + \ell) \mathbb{P}(Y = \ell) = \mathbb{P}(X = k + \ell) \mathbb{P}(X = \ell) \neq 0,
            </me>
            par hypothèse. On a en particulier, pour tout <m> k \in \mathbb{N} </m> :
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = 0) = \mathbb{P}(X = k) \mathbb{P}(X = 0),
            </me>
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = 1) = \mathbb{P}(X = k + 1) \mathbb{P}(X = 1).
            </me>
            En divisant les égalités, on obtient :
            <me>
            \frac{\mathbb{P}(X = k + 1)}{\mathbb{P}(X = k)} = \frac{\mathbb{P}(I = 1) \mathbb{P}(X = 0)}{\mathbb{P}(I = 0) \mathbb{P}(X = 1)}.
            </me>
            Ce rapport est indépendant de <m> k </m> et strictement positif. On le note <m> q </m>. La suite <m> (\mathbb{P}(X = k)) </m> est géométrique de raison <m> q </m>. Pour tout <m> k \in \mathbb{N} </m>, on a <m> \mathbb{P}(X = k) = \mathbb{P}(X = 0) q^k </m>. La série <m> \sum \mathbb{P}(X = k) </m> converge et a pour somme 1 donc <m> q \lt 1 </m> et <m> \mathbb{P}(X = 0) = 1 - q </m>. En posant <m> p = 1 - q </m>, on a le résultat voulu.
            </p>
        </solution>
    </task>
</exercise>

    <exercise>
        <title>Une caractérisation de la loi de Poisson</title>
    

        <introduction>
            <p>
                On considère une variable aléatoire discrète <m>N</m> sur l'espace probabilisé <m>(\Omega, \mathcal{A}, \PP</m> telle que <m>N(\Omega)=\N</m> et <m>\Pr{N=n} \neq 0</m> pour tout <m>n \in \N</m>.
                Si la variable aléatoire <m>N</m> prend la valeur <m>n</m>, on procède à une succession de <m>n</m> épreuves de Bernoulli indépendantes de paramètre <m>p \in] 0,1[</m>.
                On note <m>S</m> et <m>E</m> les variables aléatoires représentant respectivement le nombre de succès et d'échecs dans ces <m>n</m> épreuves.
            </p>
        </introduction>


        <task>
            <p>
                Montrer que si <m>N</m> suit une loi de Poisson de paramètre <m>\lambda>0</m>, les variables <m>S</m> et <m>E</m> suivent aussi des lois de Poisson dont on déterminera les paramètres.
                Montrer que les variables <m>E</m> et <m>S</m> sont indépendantes.
            </p>
        </task>


        <task>
            <p>
                Montrer réciproquement que si <m>S</m> et <m>E</m> sont indépendantes, alors <m>N</m> suit une loi de Poisson.
                Pour cela, on montrera :
            </p>

            <ul>
                <li>
                    <p>
                        qu'il existe deux suites <m>\left(u_{n}\right)_{n \in \N}</m> et <m>\left(v_{n}\right)_{n \in \N}</m> telles que :
                        <me>
                            \forall(m, n) \in \N^{2} \quad(m+n) ! \Pr{N=m+n}=u_{m} v_{n}
                        </me>
                    </p>
                </li>

                <li>
                    <p>
                        que les suites <m>\left(u_{n}\right)_{n \in \N}</m> et <m>\left(v_{n}\right)_{n \in \N}</m> sont géométriques.
                    </p>
                </li>
            </ul>
        </task>
    </exercise>

    <exercise>
        <title>Lancer de pièce jusqu'au premier pile</title>

            <introduction>
                <p>
                    On lance une pièce de monnaie jusqu'à l'obtention du premier pile, la probabilité d'obtenir pile étant <m>p \in] 0,1[</m>.
                    Soit <m>N</m> la variable aléatoire représentant le nombre de lancers nécessaires.
                    Si <m>N=n</m>, on relance ensuite <m>n</m> fois la pièce et on appelle <m>X</m> la variable aléatoire représentant le nombre de piles obtenu.
                </p>
            </introduction>


            <task>
                <p>
                    Déterminer la loi de <m>N</m>, celle du couple <m>(N, X)</m>, puis la loi de <m>X</m>.
                </p>
            </task>


            <task>
                <p>
                    Montrer que <m>X</m> a même loi que le produit de deux variables indépendantes <m>Y</m> et <m>Z</m> telles que <m>Y</m> suive une loi de Bernoulli et <m>Z</m> une loi géométrique de même paramètre.
                </p>
            </task>


            <task>
                <p>
                    En déduire l'espérance et la variance de <m>X</m>.
                </p>
            </task>
    </exercise>

   <exercise>
    <title>Loi Zeta</title>
        <introduction>
            <p>
                Soit <m>P</m> l'ensemble des nombres premiers. Pour <m>s \gt 1</m>, on note <m>\zeta(s) = \sum_{n \geq 1} n^{-s}</m> et <m>X</m> une variable aléatoire à valeurs dans <m>\mathbb{N}^*</m> dont la loi est définie par :
                <me>
                    \forall n \in \mathbb{N}^* \quad \mathbb{P}(X = n) = \frac{n^{-s}}{\zeta(s)}.
                </me>
            </p>
        </introduction>

            <task>
                <statement>
                    <p>
                        Justifier qu'on définit bien ainsi la loi d'une variable aléatoire.
                    </p>
                </statement>
                <solution>
                    <p>
                         Les réels <m>\frac{n^{-s}}{\zeta(s)}</m>, pour <m>n \in \mathbb{N}^*</m>, sont positifs et <m>\sum_{n=1}^{+\infty} \frac{n^{-s}}{\zeta(s)} = 1</m> par définition, donc on définit bien la loi d'une variable aléatoire.
                    </p>
                </solution>
            </task>

            <task>
                <statement>
                    <p>
                        Pour tout <m>n \in \mathbb{N}^*</m>, on considère <m>A_n : « n \text{ divise } X »</m>. Montrer que <m>(A_p)_{p \in P}</m> est une famille d'événements indépendants. En déduire une preuve probabiliste de :
                    <me>
                        \prod_{p \in P} \left( 1 - \frac{1}{p^s} \right) = \frac{1}{\zeta(s)}.
                    </me>
                    </p>
                </statement>
                <solution>
                    <p>
                        Pour tout <m>n \in \mathbb{N}^*</m>, <m>A_n = \bigcup_{j \in \mathbb{N}^*} \{X = nj\}</m> est un événement et :
                        <me>
                            \mathbb{P}(A_n) = \sum_{j=1}^{+\infty} \mathbb{P}(X = jn) = \frac{1}{\zeta(s)} \sum_{j=1}^{+\infty} (jn)^{-s} = \frac{n^{-s}}{\zeta(s)} \sum_{j=1}^{+\infty} j^{-s} = n^{-s}.
                        </me>
                        Soit <m>p_1, p_2, \ldots, p_k</m> des nombres premiers distincts. Ces nombres sont premiers entre eux donc, d'après le théorème de Gauss :
                        <me>
                            A_{p_1} \cap A_{p_2} \cap \ldots \cap A_{p_k} = A_{p_1 p_2 \ldots p_k}.
                        </me>
                        On en déduit :
                        <me>
                            \mathbb{P}\left(A_{p_1} \cap A_{p_2} \cap \ldots \cap A_{p_k}\right) = \mathbb{P}\left(A_{p_1 p_2 \ldots p_k}\right) = (p_1 p_2 \ldots p_k)^{-s} = \prod_{i=1}^k p_i^{-s} = \prod_{i=1}^k \mathbb{P}(A_{p_i}).
                        </me>
                        Les événements de la famille <m>(A_p)_{p \in P}</m> sont donc indépendants.

                        On en déduit que <m>{(A_p^c)}_{p \in P}</m> est aussi une famille d'événements indépendants. Notons <m>(p_n)_{n \geq 1}</m> la suite des entiers premiers rangés par ordre croissant. On a :
                        <me>
                            \prod_{p \in P} \left( 1 - \frac{1}{p^s} \right) = \prod_{n \in \mathbb{N}^*} \left( 1 - \frac{1}{p_n^s} \right) = \lim_{N \to +\infty} \prod_{n=1}^N \left( 1 - \frac{1}{p_n^s} \right).
                        </me>
                        Par indépendance des événements, on a :
                        <me>
                            \lim_{N \to +\infty} \prod_{n=1}^N \mathbb{P}\left({A_{p_n}}^c\right) = \lim_{N \to +\infty} \mathbb{P}\left(\bigcap_{n=1}^N {A_{p_n}}^c\right).
                        </me>
                        Par continuité décroissante, on a :
                        <me>
                            \lim_{N \to +\infty} \mathbb{P}\left(\bigcap_{n=1}^N {A_{p_n}}^c\right) = \mathbb{P}\left(\bigcap_{n=1}^{+\infty} {A_{p_n}}^c\right).
                        </me>
                        Mais <m>\bigcap_{n=1}^{+\infty} {A_{p_n}}^c = \{1\}</m>, car 1 est le seul entier qui n'ait pas de diviseur premier. Comme <m>\mathbb{P}(\{1\}) = \frac{1}{\zeta(s)}</m>, on en déduit :
                        <me>
                            \prod_{p \in P} \left( 1 - \frac{1}{p^s} \right) = \frac{1}{\zeta(s)}.
                        </me>
                    </p>
                </solution>
            </task>
            <task>
                <statement>
                    <p>
                        Montrer que la probabilité qu'aucun carré différent de 1 ne divise <m>X</m> vaut <m>\frac{1}{\zeta(2s)}</m>.
                    </p>
                </statement>
                <solution>
                    <p>
                        Notons <m>E</m> l'événement « aucun carré différent de 1 ne divise <m>X</m> ». L'événement <m>E</m> est réalisé si, et seulement si, le carré d'aucun nombre premier ne divise <m>X</m>. On a donc <m>E = \bigcap_{p \in P} A_{p^2}</m> et toujours par indépendance des événements <m>A_p</m> :
                        <me>
                            \mathbb{P}(E) = \mathbb{P}\left(\bigcap_{p \in P} A_{p^2}\right) = \lim_{N \to +\infty} \mathbb{P}\left(\bigcap_{n=1}^N A_{p_n^2}\right) = \lim_{N \to +\infty} \prod_{n=1}^N \mathbb{P}(A_{p_n^2}).
                        </me>
                        On a :
                        <me>
                            \mathbb{P}(A_{p_n^2}) = 1 - \frac{1}{p_n^{2s}}.
                        </me>
                        Ainsi :
                        <me>
                            \mathbb{P}(E) = \lim_{N \to +\infty} \prod_{n=1}^N \left( 1 - \frac{1}{p_n^{2s}} \right) = \prod_{p \in P} \left( 1 - \frac{1}{p^{2s}} \right) = \frac{1}{\zeta(2s)}.
                        </me>
                    </p>
                </solution>
            </task>

            <task><title>Densité naturelle d'une partie de <m>\N</m>    </title>
                <statement>
                    <p>
                        Soit <m>A</m> une partie de <m>\N</m> telle que la suite <m>\Bigl(\frac1n\card\bigl(A\cap\iic{1,n}\bigr)\Bigr)_n</m> converge. On note <m>d(A)</m> sa limite et on l'appelle densité naturelle de <m>A</m>.
                    </p>
                    <p>
                        Montrer que <m>\Pr{X\in A}\underset{s\to 1}\longrightarrow d(A)</m>.
                    </p>
                </statement>
                <solution>
                    <p>
                        Traitons d'abords le cas où <m>A</m> est infini. 
                    </p><p>
                Soit <m>\epsilon\gt  0</m>, posons pour tout <m>n \in \mathbb{N}</m>, <m>c_n = \operatorname{Card}(A \cap \{1, \dots, n\})</m>. On a par hypothèse :
                <me>
                    \lim_{n \to \infty} \frac{c_n}{n} = d(A),
                </me>
                donc il existe <m>n_0 \in \mathbb{N}</m> tel que :
                <me>
                    \forall n \in \mathbb{N}, \quad n \geq n_0 \Rightarrow \left| \frac{c_n}{n} - d(A) \right| \leq \epsilon.
                </me>
            </p>
            <p>
                On écrit alors :
                <me>
                    \mathbb{P}_s(X \in A) = \frac{\sum_{a \in A} \frac{1}{a^s}}{\zeta(s)} = \frac{\sum_{\substack{a \in A \\ a\lt n_0}} \frac{1}{a^s} + \sum_{\substack{a \in A \\ a \geq n_0}} \frac{1}{a^s}}{\zeta(s)}.
                </me>
            </p>
            <p>
                On s'intéresse au terme <m>\sum_{\substack{a \in A \\ a \geq n_0}} \frac{1}{a^s}</m>. On a :
                <me>
                    \sum_{\substack{a \in A \\ a \geq n_0}} \frac{1}{a^s} = \sum_{\substack{a \in A \\ a \geq n_0}} \frac{c_a^s}{c_a^s a^s} \leq \sum_{\substack{a \in A \\ a \geq n_0}} \frac{(d(A) + \epsilon)^s}{c_a^s},
                </me>
            </p>
            <p>
                Puisque <m>A</m> est infini, soit <m>(a_n)_n</m> la suite strictement croissante de ses éléments. Il s'ensuit alors que pour <m>m \in \mathbb{N}</m>, <m>c_{a_m} = m</m>. Posons alors :
                <me>
                    n_1 = \min(\{m \in \mathbb{N}, a_m \geq n_0\}).
                </me>
            </p>
            <p>
                On a donc :
                <md>
                    <mrow> \frac1{\zeta(s)}{\sum_{\substack{a \in A \\ a \geq n_0}} \frac{1}{c_a^s}}\amp= \frac1{\zeta(s)}\biggl(\sum_{m=1}^\infty \frac{1}{m^s} - \sum_{m=1}^{n_1 - 1} \frac{1}{m^s}\biggr)
                     </mrow>
                     <mrow> \amp= \frac1{\zeta(s)}\biggl(\zeta(s) - \sum_{m=1}^{n_1 - 1} \frac{1}{m^s}\biggr)  </mrow>
                     <mrow> \amp= 1 - \frac1{\zeta(s)}\biggl(\sum_{m=1}^{n_1 - 1} \frac{1}{m^s}\biggr) \underset{s \to 1^+}{\longrightarrow} 1. </mrow>
                </md>
                car <m>\zeta(s)\underset1\sim\frac1{s-1}</m> et donc <m>\zeta(s)\underset{s\to 1}\longrightarrow 0</m>.
                D'où :
                <me>
                    \mathbb{P}_s(X \in A) \leq \frac{\sum_{\substack{a \in A \\ a\lt n_0}} \frac{1}{a^s}}{\zeta(s)} + (d(A) + \epsilon)^s \left(1 - \frac{\sum_{m=1}^{n_1 - 1} \frac{1}{m^s}}{\zeta(s)}\right) \underset{s \to 1^+}{\longrightarrow} d(A) + \epsilon.
                </me>
            </p>
            <p>
                On fait alors de même à gauche et on trouve pour <m>s</m> assez proche de 1 que :
                <me>
                    d(A) - 2\epsilon \leq \mathbb{P}_s(X \in A) \leq d(A) + 2\epsilon,
                </me>
                ainsi :
                <me>
                    \mathbb{P}_s(X \in A) \underset{s \to 1^+}{\longrightarrow} d(A).
                </me>

                    </p>

                    <p>
                        Si maintenant <m>A</m> est fini et <m>N</m> est un entier majorant de <m>A</m> alors d'un côte <m>\card A\leq N+1</m> et donc <m>c_n\leq\frac{N+1}n</m> et par suite <m>c_n\rightarrow0</m>. De l'autre 
                        <me>
                            \Pr{X\in A}\leq \Pr{X\leq N}=\frac1{\zeta(x)}\sum_{k=1}^N\frac1{k^s}\leq \frac{N}{\zeta(s)}
                        </me>
                        et donc <m>\Pr{X\in A}\underset{s\to 1}\longrightarrow0</m>.
                        
                    </p>
                </solution>
            </task>

</exercise>

    <exercise>
        <title>Taux de panne</title>

        <introduction>
            <p>
                Soit <m>X</m> une variable aléatoire discrète à valeurs dans <m>\N^{*}</m> vérifiant :
                <me>
                    \forall n \in \N^{*} \quad \Pr{X \geqslant n}>0
                </me>
                <m>X</m> représente le moment où un mécanisme tombe en panne. C'est à dire le numéro de l'instance de son cycle de fonctionnement où il tombe en panne. En principe, sous l'effet de l'usure, plus la durée de son fonctionnement est grande plus la probabilité que le mécanisme tombe en panne augmente. 
                </p> 
                <p>que On appelle taux de panne associé à <m>X</m> la suite réelle <m>\left(x_{n}\right)_{n \in \N^{*}}</m> définie par :
                <me>
                    \forall n \in \N^{*} \quad x_{n}=\Pr{X=n  \giv  X \geqslant n}
                </me>
                <m>x_n</m> est la probabilité pour que le mécanisme tombe en panne à l'instant <m>n</m> sachant qu'il a fonctionné jusqu'à cet instant. 
            </p>
            </introduction>


            <task>
                
                <statement>
                    <p>
                        Exprimer <m>p_{n}=\Pr{X=n}</m> en fonction des <m>x_{k}</m>.
                    </p>
                </statement>
                <hint>
                    <p>
                        Éviter de diviser par <m>x_n</m>. Exprimer <m>\Pr{X\geq n}</m> comme un produit de facteurs <m>(1-x_k)</m>.
                    </p>
                </hint>
                <answer>
                    <p>
                        <me>
                            \forall n \in \N^{*} \quad p_{n}=x_{n}\prod_{k=1}^{n-1}(1-x_{k})
                        </me> 
                    </p>
                </answer>

                <solution>
                    <p>
                        On a <m>(X=n)\subset (X\geqslant n)</m> donc <m>\Pr{X=n}=\Pr{X=n,X\geq n}</m>.
                        Ce qui donne
                        <men xml:id="eq-tauxpanne">
                            x_n=\Pr{X=n \giv  X\geq n}=\frac{\Pr{X=n}}{\Pr{X\geq n}}
                        </men>
                        On en déduit que 
                        <me>
                            1-x_n=\frac{\Pr{X\geq n}-\Pr{X=n}}{\Pr{X\geq n}}=\frac{\Pr{X\geq n+1}}{\Pr{X\geq n}}
                        </me>
                        Ce qui donne par télescopage 
                        <me>
                            \prod_{k=1}^{n-1}(1-x_k)=\frac{\Pr{X\geq n}}{\Pr{X\geq 1}}=\Pr{X\geq n}
                        </me>
                        La relation <xref ref="eq-tauxpanne"/> signifie que <m>p_n=x_n\Pr{X\geq n}</m> donc finalement 
                        <men xml:id="eqn-pnexpr">
                            p_n=x_n\prod_{k=1}^{n-1}(1-x_k)
                        </men>
                        
                        
                        
                        











                        
                    </p>
                </solution>
            </task>


            <task><title>Caractérisation du taux de panne</title>
                <statement>
                    <p>
                        <ol marker="1.">
                            <li>
                                <p>
                                    Montrer que <m>0 \leqslant x_{n} \lt 1</m> pour tout <m>n \in \N^{*}</m> et que la série de terme général <m>x_{n}</m> diverge.
                                </p>
                            </li>

                            <li>
                                <p>
                                    Réciproquement, soit <m>\left(x_{n}\right)_{n \in \N^{*}}</m> une suite à valeur dans <m>[0,1[</m> telle que la série de terme général <m>x_{n}</m> diverge.
                                    Montrer qu'il existe une variable aléatoire dont le taux de panne est la suite <m>\left(x_{n}\right)</m>.
                                </p>
                            </li>
                        </ol>
                    </p>
                </statement>
                <hint>
                    <p>
                        On rappelle que pour une suite <m>(p_n)_n</m> de réels positifs sommable et de somme <m>1</m>, il existe une variable aléatoire <m>X</m> telle que <m>\Pr{X=n}=p_n</m> pour tout <m>n</m>.   
                    </p>
                </hint>

                <solution>
                    <ol marker="1.">
                        <li>
                            <p>
                                Soit <m>n\in\N^*</m> et supposons que <m>x_n=1</m>.
                                Alors <m>p_n=\Pr{X\geq n}</m>, ou encore <m>\Pr{X=n}=\Pr{X\geq n}</m>.
                                Ce qui implique que <m>\Pr{X\geq n+1}=0</m> contredisant l'hypothèse faite dans l'énoncé.
                                Alors <m>x_n\lt1</m>.
                            </p>

                            <p>
                                Ensuite <m>\PP(X\geq n+1)</m> est le reste de la série convergente <m>\sum p_k</m> donc il converge vers <m>0</m>. Ce qui implique que 
                                <me>
                                \sum_{k=1}^{n}\ln(1-x_n)=\ln\bigl(\Pr{X\geq n}\bigr)\longrightarrow -\infty
                                </me>
                                La série <m>\sum x_n</m> est nécessairement divergente car dans le cas contraire <m>(x_n)_n</m> convergerait vers <m>0</m> et on aurait donc <m>-\ln(1-x_n)\sim x_n</m> ce qui impliquerait que la série <m>\sum \ln(1-x_n)</m> est convergente.  
                            </p>
                        </li>

                        <li>
                            <p> Soit <m>\sum x_n</m> une série divergente à termes dans <m>[0,1[</m> et posons <m>v_1=1</m> pour tout <m>n\geq 2</m>
                                <me>
                                    v_n=\prod_{k=1}^{n-1}(1-x_k) \qtext{et} p_n=x_nv_n
                                </me>
                            Avec ces relations on a 
                                <me>
                                    v_n-v_{n+1}=v_nx_n=p_n
                                </me>
                            On peut ensuite écrire 
                            <me>
                                \ln v_n=\sum_{k=1}^{n-1}\ln(1-x_n)\leqslant -\sum_{k=1}^{n-1} x_k
                            </me>
                            Puisque la série de réels positif<m>\sum x_n</m> est divergente positive sa suite des sommes partielles tend vers <m>+\infty</m> et on a donc 
                            <m>\ln v_n\longrightarrow-\infty</m>. Par suite <m>v_n\longrightarrow 0</m>. Puisque <m>p_n=v_n-v_{n+1}</m> alors la série <m>\sum p_n</m> est convergente de somme <m>v_1=1</m>.
                            </p>
                            <p>
                                Il existe donc une VADR <m>X</m> tel que <m>\PP(X=n)=p_n</m> pour tout <m>n\in\N^*</m>. 
                            </p>
                        </li>
                    </ol>
                </solution>
            </task>


            <task>
            <statement>
                <p>
                    Montrer que la variable <m>X</m> suit une loi géométrique si, et seulement si, son taux de panne est constant.
                </p>
            </statement>
            <solution>
                <p>
                    On suppose que <m>X</m> suit une loi géométrique de paramètre <m>p</m>. Alors pour tout <m>n\in\N^*</m> on a
                    <m>\PP(X=n)=p(1-p)^{n-1}</m>. Donc 
                    <me>\PP(X=n+1)=\sum_{k=n+1}p(1-p)^{k-1}=(1-p)^n</me>
                    Par suite 
                    <me>\Pr{X=n \giv  X\geq n}=\frac{\Pr{X=n}}{\Pr{X\geq n}}=\frac{p(1-p)^{n-1}}{(1-p)^{n-1}}=p</me>. 
                    Donc le taux de panne est constant.

                    Réciproquement, on suppose que le taux de panne est constant de valeur <m>p</m>. Alors pour tout <m>n\in\N^*</m> on a
                    <me>\PP(X=n)=x_n\prod_{k=1}^{n-1}(1-x_k)=p(1-p)^{n-1}</me>. 
                    
                    Donc <m>X</m> suit une loi géométrique de paramètre <m>p</m>.
                </p>
                <p> Noter que cela signifie que le taux de panne est constant si et seulement si les événements «le mécanisme tombe en panne à l'instant <m>n</m>» sont mutuellement indépendants et ont tous la même probabilité. Il n'y a aucun effet d'usure.
                </p>
            </solution>
            </task>
    </exercise>


   <exercise>
    <title>Maximums et minimums provisoires</title>
    <introduction>
        <p>
            Soit <m>n \in \mathbb{N}^*</m>. On désigne par <m>\Omega</m> l'ensemble des permutations de <m>[1, n]</m>. On munit <m>\Omega</m> de la probabilité uniforme. Pour <m>\sigma \in \Omega</m> et <m>i \in [1, n]</m>, on dit que <m>\sigma(i)</m> est un <em>maximum provisoire</em> (resp. <em>minimum provisoire</em>) de <m>\sigma</m> si :
            <me>
                \sigma(i) = \max(\sigma(1), \sigma(2), \ldots, \sigma(i)) \quad \text{(resp. } \sigma(i) = \min(\sigma(1), \sigma(2), \ldots, \sigma(i))\text{)}.
            </me>
            On désigne par <m>X_n</m> (resp. <m>Y_n</m>) les variables aléatoires représentant le nombre de maximums (resp. minimums) provisoires des permutations de <m>[1, n]</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que les variables <m>X_n</m> et <m>Y_n</m> ont même loi.
            </p>
        </statement>
        <solution>
            <p>
                On observe qu'en <m>1</m>, il y a toujours un maximum et un minimum provisoire, et donc que <m>X_n</m> et <m>Y_n</m> sont à valeurs dans <m>[1, n]</m>.
                L'application <m>f : \Omega \to \Omega</m> qui à la permutation <m>\sigma</m> associe la permutation <m>\sigma' : k \mapsto n + 1 - \sigma(k)</m> est clairement bijective. Pour <m>i \in [1, n]</m>, <m>\sigma(i)</m> est un maximum provisoire de <m>\sigma</m> si, et seulement si, <m>\sigma'(i)</m> est un minimum provisoire de <m>\sigma</m>. En effet :
                <me>
                    \sigma(i) = \max(\sigma(1), \ldots, \sigma(i)) \quad \text{équivaut à} \quad n + 1 - \sigma'(i) = \max(n + 1 - \sigma'(1), \ldots, n + 1 - \sigma'(i)),
                </me>
                ce qui équivaut à :
                <me>
                    \sigma'(i) = \min(\sigma'(1), \ldots, \sigma'(i)).
                </me>
                On en déduit que, pour tout <m>k \in [1, n]</m>,
                <me>
                    \sigma \in \{X_n = k\} \quad \text{si et seulement si} \quad \sigma' \in \{Y_n = k\}.
                </me>
                Comme <m>f</m> est bijective, on a <m>\text{card}(\{X_n = k\}) = \text{card}(\{Y_n = k\})</m>, et donc <m>\mathbb{P}(X_n = k) = \mathbb{P}(Y_n = k)</m>, car <m>\Omega</m> est muni de la probabilité uniforme.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>X_3</m>, son espérance et sa variance.
            </p>
        </statement>
        <solution>
            <p>
                <ul>
                    <li>
                        La permutation <m>\sigma</m> est dans <m>\{X_3 = 1\}</m> si, et seulement si, <m>\sigma(1) = 3</m>. On en déduit <m>\mathbb{P}(X_3 = 1) = \frac{1}{3}</m>.
                    </li>
                    <li>
                        On a <m>\{X_3 = 3\} = \{\text{Id}_{[1,3]}\}</m>. On en déduit <m>\mathbb{P}(X_3 = 3) = \frac{1}{6}</m>.
                    </li>
                    <li>
                        Enfin, <m>\mathbb{P}(X_3 = 2) = 1 - \frac{1}{3} - \frac{1}{6} = \frac{1}{2}</m>.
                    </li>
                </ul>
                On obtient :
                <me>
                    \mathbb{E}(X_3) = \frac{11}{6} \quad \text{et} \quad \mathbb{V}(X_3) = \frac{17}{36}.
                </me>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi du couple <m>(X_3, Y_3)</m> et sa covariance.
            </p>
        </statement>
        <solution>
            <p>
                Pour <m>(k, \ell) \in [1, 3]^2</m>, on a <m>\mathbb{P}(X_3 = k, Y_3 = \ell) = 0</m> si <m>k + \ell \geq 4</m>. En effet, sauf pour <m>i = 1</m>, <m>\sigma(i)</m> ne peut pas être à la fois un maximum provisoire et un minimum provisoire. On obtient :
                <me>
                    \begin{array}{|c|c|c|c|}
                        \hline
                        X  \amp   Y  \amp   1  \amp   2  \amp   3 \\
                        \hline
                        1  \amp   0  \amp   \frac{1}{6}  \amp   \frac{1}{6} \\
                        \hline
                        2  \amp   \frac{1}{6}  \amp   \frac{1}{3}  \amp   0 \\
                        \hline
                        3  \amp   \frac{1}{6}  \amp   0  \amp   0 \\
                        \hline
                    \end{array}
                </me>
                On trouve :
                <me>
                    \mathbb{E}(X_3 Y_3) = 3 \quad \text{et} \quad \text{Cov}(X_3, Y_3) = -\frac{13}{36}.
                </me>
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Formule du crible</title>
    <task>
        <statement>
            <p>
                Soit <m> A_1, A_2, \ldots, A_n </m> des événements d'un espace probabilisé <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>. Montrer que :
                <me>
                1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}).
                </me>
                En déduire la formule du crible :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \sum_{k=1}^n \left( (-1)^{k-1} \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right) \right).
                </me>
            </p>
        </statement>
        <solution>
            <p>
                Pour montrer l'égalité <m> 1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}) </m>, on observe que :
                <ul>
                    <li>
                        Si <m> \omega \in \bigcup_{i=1}^n A_i </m>, alors il existe au moins un <m> i </m> tel que <m> \omega \in A_i </m>, donc <m> 1_{A_i}(\omega) = 1 </m>. Ainsi, <m> \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 0 </m>, et donc <m> 1 - \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 1 </m>.
                    </li>
                    <li>
                        Si <m> \omega \notin \bigcup_{i=1}^n A_i </m>, alors <m> \omega \notin A_i </m> pour tout <m> i </m>, donc <m> 1_{A_i}(\omega) = 0 </m>. Ainsi, <m> \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 1 </m>, et donc <m> 1 - \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 0 </m>.
                    </li>
                </ul>
                On en déduit que <m> 1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}) </m>.

                Pour obtenir la formule du crible, on utilise l'espérance de cette égalité :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \mathbb{E}\left( 1_{\bigcup_{i=1}^n A_i} \right) = 1 - \mathbb{E}\left( \prod_{i=1}^n (1 - 1_{A_i}) \right).
                </me>
                En développant le produit, on obtient :
                <me>
                \prod_{i=1}^n (1 - 1_{A_i}) = \sum_{k=0}^n (-1)^k \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \prod_{i \in I} 1_{A_i}.
                </me>
                En prenant l'espérance, on a :
                <me>
                \mathbb{E}\left( \prod_{i=1}^n (1 - 1_{A_i}) \right) = \sum_{k=0}^n (-1)^k \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right).
                </me>
                En substituant dans l'expression précédente, on obtient la formule du crible :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \sum_{k=1}^n (-1)^{k-1} \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right).
                </me>
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Soit <m> n \in \mathbb{N}^* </m> et <m> (X_k)_{k \in \mathbb{N}^*} </m> une suite de variables indépendantes d'un espace probabilisé <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>, suivant toutes la loi uniforme sur <m>[1, n]</m>. On note <m> X </m> la variable aléatoire égale au nombre de tirages nécessaires pour obtenir tous les numéros entre 1 et <m> n </m> au moins une fois (et à <m> +\infty </m> si on n'obtient jamais les <m> n </m> numéros). Pour <m> j \in [1, n] </m> et <m> m \in \mathbb{N} </m>, on note <m> B_{j,m} </m> l'événement : « au bout de <m> m </m> tirages, le numéro <m> j </m> n'est pas encore apparu ».
            </p>
            <ol>
                <li>
                    Calculer <m> \mathbb{P}(B_{j_1,m} \cap B_{j_2,m} \cap \cdots \cap B_{j_k,m}) </m>, où <m> j_1, j_2, \ldots, j_k </m> sont des indices distincts compris entre 1 et <m> n </m>.
                </li>
                <li>
                    En déduire que <m> \mathbb{P}(X\gt m) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m </m>. Calculer <m> \lim_{m \to +\infty} \mathbb{P}(X\gt m) </m>. Interpréter.
                </li>
                <li>
                    Montrer que <m> \mathbb{E}(X) = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k} </m>, en utilisant l'exercice 18 de la page 911.
                </li>
                <li>
                    Montrer que <m> \mathbb{E}(X) = n \left( 1 + \frac{1}{2} + \cdots + \frac{1}{n} \right) </m>. En déduire un équivalent de <m> \mathbb{E}(X) </m> quand <m> n </m> tend vers <m> +\infty </m>.
                </li>
            </ol>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        Pour <m> j_1, j_2, \ldots, j_k </m> des indices distincts compris entre 1 et <m> n </m>, on a :
                        <me>
                        \mathbb{P}(B_{j_1,m} \cap B_{j_2,m} \cap \cdots \cap B_{j_k,m}) = \left( \frac{n - k}{n} \right)^m.
                        </me>
                        En effet, chaque tirage a une probabilité <m> \frac{n - k}{n} </m> de ne pas tomber sur l'un des <m> k </m> numéros <m> j_1, j_2, \ldots, j_k </m>.
                    </li>
                    <li>
                        En appliquant la formule du crible, on a :
                        <me>
                        \mathbb{P}(X\gt m) = \mathbb{P}\left( \bigcup_{j=1}^n B_{j,m} \right) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m.
                        </me>
                        La limite <m> \lim_{m \to +\infty} \mathbb{P}(X\gt m) = 0 </m> car <m> \left( \frac{n - k}{n} \right)^m </m> tend vers 0 pour tout <m> k \geq 1 </m>. Cela signifie qu'il est presque sûr que tous les numéros seront obtenus en un nombre fini de tirages.
                    </li>
                    <li>
                        En utilisant l'exercice 18 de la page 911, on a :
                        <me>
                        \mathbb{E}(X) = \sum_{m=0}^{+\infty} \mathbb{P}(X\gt m) = \sum_{m=0}^{+\infty} \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m.
                        </me>
                        En intervertissant les sommes, on obtient :
                        <me>
                        \mathbb{E}(X) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \sum_{m=0}^{+\infty} \left( \frac{n - k}{n} \right)^m = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \frac{1}{1 - \frac{n - k}{n}} = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k}.
                        </me>
                    </li>
                    <li>
                        On a :
                        <me>
                        \mathbb{E}(X) = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k} = n \left( 1 + \frac{1}{2} + \cdots + \frac{1}{n} \right).
                        </me>
                        Quand <m> n </m> tend vers <m> +\infty </m>, on a <m> \mathbb{E}(X) \sim n \ln n </m>.
                    </li>
                </ol>
            </p>
        </solution>
    </task>
</exercise>



<exercise>
    <title>Variables aléatoires uniformes et Poisson</title>
    <introduction>
        <p>
            Soient un entier <m>n \geqslant 1</m> et une suite <m>(U_{k})_{k \in \N^{*}}</m> de variables aléatoires indépendantes et de même loi uniforme sur <m>\llbracket 1, n \rrbracket</m>. Pour tout <m>i \in \llbracket 1, n \rrbracket</m>, on définit :
            <me>
                X_{i}^{(0)} = 0 \quad \text{et} \quad X_{i}^{(m)} = \card\{k \in \llbracket 1, m \rrbracket \mid U_{k} = i\} \quad \forall m \geqslant 1.
            </me>
        </p>
        </introduction>
        <task>
            <p>
                Quelle est la loi de <m>X_{i}^{(m)}</m> pour <m>i \in \llbracket 1, n \rrbracket</m> et <m>m \geqslant 1</m> ?
            </p>
        </task>
        <task>
            <p>
                Soit <m>m \geqslant 1</m> et <m>(i, j) \in \llbracket 1, n \rrbracket^{2}</m> avec <m>i \neq j</m>. Calculer la covariance des variables aléatoires <m>X_{i}^{(m)}</m> et <m>X_{j}^{(m)}</m>. Sont-elles indépendantes ?
            </p>
        </task>
        <task>
            <p>
                Soit <m>\lambda\gt0</m> et <m>N</m> une variable aléatoire suivant une loi de Poisson de paramètre <m>\lambda</m>, indépendante des variables <m>U_{k}</m>. On pose :
                <me>
                    \forall i \in \llbracket 1, n \rrbracket \quad Y_{i} = X_{i}^{(N)}.
                </me>
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Déterminer, en fonction de <m>\lambda</m> et <m>n</m>, la loi de <m>Y_{i}</m> pour tout <m>i \in \llbracket 1, n \rrbracket</m>.
                    </p>
                </li>
                <li>
                    <p>
                        Déterminer la loi conjointe de <m>(Y_{1}, \ldots, Y_{n})</m>.
                    </p>
                </li>
            </ol>
        </task>
</exercise>


<exercise xml:id="centrale_2015">
    <title>Centrale 2015</title>
    
    <task>
        <statement>
            <p>Soit <m>(\Omega, A, P)</m> un espace probabilisé et <m>(E_n)_{n \in \mathbb{N}}</m> 
            une suite d’événements.</p>
            <p>On suppose que la série suivante converge :</p>
            <me>
                \sum_{n=0}^{+\infty} \PP(E_n) \lt +\infty.
            </me>
        </statement>
        <solution>
            <p>La convergence de cette série joue un rôle essentiel dans l'analyse des probabilités des événements.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>On note <m>1_X</m> la fonction indicatrice d’un ensemble <m>X</m>. 
            Soit <m>Z = \sum_{n=0}^{+\infty} 1_{E_n}</m> (avec la convention <m>Z = +\infty</m> 
            si la série diverge). Prouver que <m>Z</m> est une variable aléatoire discrète.</p>
        </statement>
        <solution>
            <p>On montre que <m>Z</m> prend ses valeurs dans <m>\mathbb{N} \cup \{+\infty\}</m>, 
            qui est un ensemble dénombrable. Pour <m>n \in \mathbb{N}</m>, on a :</p>
            <me>
                \{Z = n\} = \bigcup_{I \in P_n(\mathbb{N})} 
                \left( \bigcap_{k \in I} E_k \cap \bigcap_{k \in \mathbb{N} \setminus I} E_k^c \right).
            </me>
            <p>Chaque ensemble à l'intérieur de cette union est un événement, donc <m>\{Z = n\}</m> 
            est un événement. De même, on montre que <m>\{Z = +\infty\}</m> est un événement, donc 
            <m>Z</m> est bien une variable aléatoire discrète.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>Soit <m>F = \{\omega \in \Omega \mid \omega</m> appartient à un nombre fini de 
            <m>E_n</m> (pour <m>n \in \mathbb{N}</m>)}. Prouver que <m>F</m> est un événement 
            et que <m>\PP(F) = 1</m>.</p>
        </statement>
        <solution>
            <p>La série <m>\sum \PP(E_n)</m> converge, donc d'après le premier lemme de <em>Borel-Cantelli</em>,</p>
            <me>
                P\left( \bigcap_{n \in \mathbb{N}} \bigcup_{p \geq n} E_p \right) = 0.
            </me>
            <p>Cela signifie que <m>\PP(Z = +\infty) = 0</m>. Donc, presque sûrement, 
            <m>\omega</m> appartient à un nombre fini d'événements <m>E_n</m>, ce qui montre que 
            <m>F</m> est un événement et que <m>\PP(F) = 1</m>.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>Prouver que <m>Z</m> est d'espérance finie.</p>
        </statement>
        <solution>
            <p>On définit <m>Z_n = \sum_{i=0}^{n} 1_{E_i}</m>. Alors :</p>
            <me>
                E(Z_n) = \sum_{i=0}^{n} \PP(E_i).
            </me>
            <p>Par passage à la limite, et en utilisant la convergence de la série :</p>
            <me>
                E(Z) = \sum_{i=0}^{+\infty} \PP(E_i).
            </me>
            <p>Ainsi, <m>Z</m> est bien d'espérance finie.</p>
        </solution>
    </task>
</exercise>




<exercise>
    <title>Marche aléatoire dans <m>\mathbb{Z}</m></title>
    <introduction>
        <p>
            Soit <m>(X_{n})_{n \in \N^{*}}</m> une suite de variables aléatoires, sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \PP</m>, indépendantes et de même loi définie par :
            <me>
                \Pr{X_{n} = 1} = p \quad \text{et} \quad \Pr{X_{n} = -1} = 1 - p,
            </me>
            où <m>p \in [0, 1]</m>. On pose <m>S_{0} = 0</m> et, pour tout <m>n \in \N^{*}</m>, <m>S_{n} = \sum_{k=1}^{n} X_{k}</m>. La suite <m>(S_{n})</m> est appelée marche aléatoire dans <m>\mathbb{Z}</m>.
        </p>
        </introduction>
        <task>
            <p>
                Déterminer <m>u_{n} = \Pr{S_{n} = 0}</m> pour tout <m>n \in \N</m>.
            </p>
        </task>
        <task>
            <p>
                On note <m>f(x)</m> la somme de la série entière <m>\sum u_{n} x^{n}</m>. Montrer que :
                <me>
                    \forall x \in ]-1, 1[ \quad f(x) = \frac{1}{\sqrt{1 - 4 p (1 - p) x^{2}}}.
                </me>
            </p>
        </task>
        <task>
            <p>
                Pour tout entier naturel non nul <m>k</m>, on note <m>A_{k}</m> l'événement « le mobile retourne pour la première fois à l'origine au bout de <m>k</m> déplacements », c'est-à-dire :
                <me>
                    A_{k} = (S_{k} = 0) \cap \left(\bigcap_{i=1}^{k-1} (S_{i} \neq 0)\right).
                </me>
                On pose <m>v_{k} = \Pr{A_{k}}</m> pour tout <m>k \geqslant 1</m> et <m>v_{0} = 0</m>.
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que, pour tout entier naturel <m>n</m> non nul, on a :
                        <me>
                            (S_{n} = 0) = \sum_{k=1}^{n} \Pr{(S_{n} = 0) \cap A_{k}}.
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire que, pour tout entier naturel non nul <m>n</m>, on a :
                        <me>
                            u_{n} = \sum_{k=0}^{n} u_{n - k} v_{k}.
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>


<exercise>
    <title>Loi faible des grands nombres dans <m>L^1</m></title>
    <p> </p>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \geq 1}</m> une suite de variables aléatoires réelles discrètes, deux à deux indépendantes, de même loi, possédant une espérance finie <m>m</m>. On pose, pour tout <m>n \in \mathbb{N}^*</m>, <m>Y_n = \frac{X_1 + \cdots + X_n}{n}</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Dans les deux premières questions, on suppose <m>m = 0</m>.
                <ol>
                    <li>
                        <p>Soit <m>\varepsilon \gt 0</m>.
                        <ol>
                            <li>
                                Pour <m>c \gt 0</m>, on définit <m>g : \mathbb{R} \to \mathbb{R}</m> par :
                                <me>
                                    g(x) = 
                                    \begin{cases} 
                                        x \amp \text{si } |x| \leq c \\
                                        0 \amp \text{sinon}.
                                    \end{cases}
                                </me>
                                Montrer que la variable aléatoire <m>g(X_1)</m> est d'espérance finie et que l'on peut choisir <m>c</m> tel que <m>\mathbb{E}(|g(X_1) - X_1|) \leq \frac{\varepsilon}{2}</m>.
                            </li>
                            <li>
                                On pose <m>a = \mathbb{E}(g(X_1))</m>. Montrer que :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon.
                                </me>
                            </li>
                            <li>
                                On pose, pour tout <m>n \in \mathbb{N}^*</m>, <m>U_n = g(X_n) - a</m> et <m>Y_n' = \frac{U_1 + \cdots + U_n}{n}</m>. Justifier que les variables <m>U_n</m> admettent un moment d'ordre 2. Montrer que <m>\lim_{n \to +\infty} \mathbb{V}(Y_n') = 0</m>. En déduire que <m>\lim_{n \to +\infty} \mathbb{E}(|Y_n|) = 0</m>.
                            </li>
                        </ol></p>
                    </li>
                    <li>
                        Montrer que, pour tout <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \lim_{n \to +\infty} \mathbb{P}(|Y_n| \geq \varepsilon) = 0.
                        </me>
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        <ol>
                            <li>
                                La fonction <m>g</m> est bornée par <m>c</m>, donc <m>g(X_1)</m> est d'espérance finie. De plus, on a :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1|) = \mathbb{E}(|X_1| \mathbf{1}_{\{|X_1| \gt c\}}).
                                </me>
                                Comme <m>\mathbb{E}(|X_1|) \lt +\infty</m>, on peut choisir <m>c</m> suffisamment grand pour que <m>\mathbb{E}(|X_1| \mathbf{1}_{\{|X_1| \gt c\}}) \leq \frac{\varepsilon}{2}</m>.
                            </li>
                            <li>
                                On a :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1 - a|) \leq \mathbb{E}(|g(X_1) - X_1|) + |a|.
                                </me>
                                Comme <m>a = \mathbb{E}(g(X_1))</m> et <m>|a| \leq \mathbb{E}(|g(X_1)|) \leq c</m>, on peut choisir <m>c</m> suffisamment grand pour que <m>\mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon</m>.
                            </li>
                            <li>
                                Les variables <m>U_n</m> sont centrées et admettent un moment d'ordre 2 car <m>g(X_n)</m> est bornée et <m>X_n</m> a une espérance finie. On a :
                                <me>
                                    \mathbb{V}(Y_n') = \frac{\mathbb{V}(U_1)}{n} \to 0 \quad \text{quand} \quad n \to +\infty.
                                </me>
                                Par l'inégalité de Markov, on a :
                                <me>
                                    \mathbb{E}(|Y_n|) \leq \mathbb{E}(|Y_n'|) + \mathbb{E}(|Y_n - Y_n'|) \leq \sqrt{\mathbb{V}(Y_n')} + \mathbb{E}(|Y_n - Y_n'|).
                                </me>
                                Comme <m>\mathbb{E}(|Y_n - Y_n'|) \leq \mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon</m>, on obtient :
                                <me>
                                    \lim_{n \to +\infty} \mathbb{E}(|Y_n|) = 0.
                                </me>
                            </li>
                        </ol>
                    </li>
                    <li>
                        Pour tout <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \mathbb{P}(|Y_n| \geq \varepsilon) \leq \frac{\mathbb{E}(|Y_n|)}{\varepsilon} \to 0 \quad \text{quand} \quad n \to +\infty.
                        </me>
                    </li>
                </ol>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                On ne suppose plus <m>m = 0</m>. Montrer que l'on a :
                <me>
                    \forall \varepsilon \gt 0, \quad \lim_{n \to +\infty} \mathbb{P}(|Y_n - m| \geq \varepsilon) = 0.
                </me>
            </p>
        </statement>
        <solution>
            <p>
                On pose <m>Z_n = Y_n - m</m>. Alors <m>Z_n</m> est la moyenne des variables <m>X_n - m</m>, qui sont centrées. D'après la question précédente, on a :
                <me>
                    \lim_{n \to +\infty} \mathbb{P}(|Z_n| \geq \varepsilon) = 0.
                </me>
                Par conséquent :
                <me>
                    \lim_{n \to +\infty} \mathbb{P}(|Y_n - m| \geq \varepsilon) = 0.
                </me>
            </p>
        </solution>
    </task>
</exercise>



<exercise>
    <title>Modèle de Galton-Watson</title>
    <introduction>
        
            <p>
                On observe des virus qui se reproduisent tous selon la même loi avant de mourir : un virus donne naissance en une journée à <m>X</m> virus, où <m>X</m> est une variable aléatoire à valeurs dans <m>\mathbb{N}</m>. Pour tout <m>k \in \mathbb{N}</m>, on note <m>\PP(X = k) = p_k</m>. On suppose <m>p_1\gt 0</m> et <m>p_0 + p_1\lt 1</m>. On note <m>f</m> la fonction génératrice de <m>X</m>.
            </p>
            <p>
                On part au jour zéro de <m>X_0 = 1</m> virus. Au premier jour, on a donc <m>X_1</m> virus, où <m>X_1</m> suit la loi de <m>X</m> ; chacun de ces <m>X_1</m> virus évolue alors indépendamment des autres virus et se reproduit selon la même loi avant de mourir : cela conduit à avoir <m>X_2</m> virus au deuxième jour ; et le processus continue de la sorte. On note <m>u_n = \PP(X_n = 0)</m>.
            </p>
        </introduction>
    <task>
        <statement>
            <p>
                Calculer <m>u_0</m> et <m>u_1</m>.
            </p>
        </statement>
        <solution>
            <p>
                Par définition, <m>u_0 = \PP(X_0 = 0)</m>. Comme <m>X_0 = 1</m>, on a :
                <me>
                    u_0 = 0.
                </me>
                Pour <m>u_1</m>, on a <m>u_1 = \PP(X_1 = 0)</m>. Comme <m>X_1</m> suit la loi de <m>X</m>, on a :
                <me>
                    u_1 = \PP(X = 0) = p_0.
                </me>
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Montrer que la suite <m>(u_n)_{n \in \mathbb{N}}</m> est convergente.
            </p>
        </statement>
        <solution>
            <p>
                La suite <m>(u_n)</m> est croissante car si <m>X_n = 0</m>, alors <m>X_{n+1} = 0</m>. Ainsi, <m>u_n \leq u_{n+1}</m>. De plus, la suite est majorée par 1, car <m>u_n = \PP(X_n = 0) \leq 1</m>. Par le théorème de la convergence monotone, la suite <m>(u_n)</m> est convergente.
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Montrer que pour tout entier <m>n \geq 0</m>, on a <m>u_{n+1} = f(u_n)</m>.
            </p>
        </statement>
        <solution>
            <p>
                On écrit, avec la formule des probabilités totales :
                <me>
                    u_{n+1} = \PP(X_{n+1} = 0) = \sum_{k=0}^{+\infty} \Pr{X_{n+1} = 0 \giv X_1 = k} \P\PP(X_1 = k).
                </me>
                Si <m>X_1 = k</m>, alors <m>X_{n+1}</m> est la somme de <m>k</m> variables indépendantes de même loi que <m>X_n</m>. Ainsi :
                <me>
                    \PP(X_{n+1} = 0 \mid X_1 = k) = \PP(X_n = 0)^k = u_n^k.
                </me>
                On obtient donc :
                <me>
                    u_{n+1} = \sum_{k=0}^{+\infty} u_n^k p_k = f(u_n),
                </me>
                où <m>f</m> est la fonction génératrice de <m>X</m>.
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Que peut-on dire de la limite de <m>(u_n)_{n \in \mathbb{N}}</m> ? Discuter selon la valeur de <m>E(X)</m>. Interpréter le résultat.
            </p>
        </statement>
        <solution>
            <p>
                La limite <m>u</m> de la suite <m>(u_n)</m> vérifie <m>u = f(u)</m>, car <m>u_{n+1} = f(u_n)</m> et <m>f</m> est continue. On a donc :
                <me>
                    u = f(u).
                </me>
                La fonction <m>f</m> est convexe et croissante sur <m>[0, 1]</m>, avec <m>f(0) = p_0</m> et <m>f(1) = 1</m>. On distingue deux cas :
                <ul>
                    <li>
                        Si <m>E(X) \leq 1</m>, alors <m>f(u) = u</m> a une unique solution <m>u = 1</m>. Ainsi, la probabilité que la population s'éteigne est 1.
                    </li>
                    <li>
                        Si <m>E(X)\gt 1</m>, alors <m>f(u) = u</m> a deux solutions : <m>u = 1</m> et une autre solution <m>u = \ell \in (0, 1)</m>. La probabilité que la population s'éteigne est <m>\ell</m>.
                    </li>
                </ul>
                Interprétation : Si l'espérance de reproduction est faible (<m>E(X) \leq 1</m>), la population s'éteint presque sûrement. Si elle est forte (<m>E(X)\gt 1</m>), il y a une probabilité non nulle que la population survive indéfiniment.
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Somme aléatoire de variables aléatoires</title>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \geq 1}</m> une suite de variables aléatoires réelles discrètes, toutes de même loi, et <m>N</m> une variable aléatoire à valeurs dans <m>\mathbb{N}</m>. On suppose que <m>N</m> et les variables <m>X_n</m>, pour <m>n \in \mathbb{N}^*</m>, forment une suite de variables aléatoires indépendantes. On pose :
            <me>
                \forall n \in \mathbb{N}^*, \quad S_n = \sum_{k=1}^n X_k \quad \text{et} \quad S_0 = 0.
            </me>
        </p>
    </introduction>
    <task>
        <statement>
            <p>
                Montrer que <m>S_N</m> est une variable aléatoire.
            </p>
        </statement>
        <solution>
            <p>
                On a <m>S_N(\Omega) \subset \{0\} \cup \bigcup_{n \in \mathbb{N}^*} S_n(\Omega)</m>. Les ensembles <m>S_n(\Omega)</m> sont au plus dénombrables, donc leur union est également au plus dénombrable. Ainsi, <m>S_N(\Omega)</m> est au plus dénombrable. De plus, pour tout <m>x \in S_N(\Omega)</m>, on a :
                <me>
                    \{S_N = x\} = \bigcup_{n \in \mathbb{N}} \{S_n = x\} \cap \{N = n\},
                </me>
                qui est une union dénombrable d'événements. Par conséquent, <m>S_N</m> est une variable aléatoire.
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Déterminer la loi de <m>S_N</m> dans les cas suivants :
                <ol>
                    <li>
                        Les <m>X_k</m> suivent la loi de Bernoulli de paramètre <m>p</m> et <m>N</m> suit la loi de Poisson de paramètre <m>\lambda</m>.
                    </li>
                    <li>
                        Les <m>X_k</m> suivent la loi géométrique de paramètre <m>p</m> et <m>N</m> suit la loi géométrique de paramètre <m>p'</m>.
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        Si les <m>X_k</m> suivent la loi de Bernoulli de paramètre <m>p</m> et <m>N</m> suit la loi de Poisson de paramètre <m>\lambda</m>, alors <m>S_N</m> suit la loi de Poisson de paramètre <m>\lambda p</m>. En effet, pour tout <m>k \in \mathbb{N}</m>, on a :
                        <me>
                            \mathbb{P}(S_N = k) = \sum_{n=0}^{+\infty} \mathbb{P}(S_n = k) \mathbb{P}(N = n) = \sum_{n=k}^{+\infty} \binom{n}{k} p^k (1-p)^{n-k} e^{-\lambda} \frac{\lambda^n}{n!}.
                        </me>
                        En simplifiant, on obtient :
                        <me>
                            \mathbb{P}(S_N = k) = e^{-\lambda p} \frac{(\lambda p)^k}{k!}.
                        </me>
                    </li>
                    <li>
                        Si les <m>X_k</m> suivent la loi géométrique de paramètre <m>p</m> et <m>N</m> suit la loi géométrique de paramètre <m>p'</m>, alors <m>S_N</m> suit la loi géométrique de paramètre <m>p p'</m>. En effet, pour tout <m>k \in \mathbb{N}^*</m>, on a :
                        <me>
                            \mathbb{P}(S_N = k) = \sum_{n=1}^{+\infty} \mathbb{P}(S_n = k) \mathbb{P}(N = n) = \sum_{n=1}^k \binom{k-1}{n-1} p^n (1-p)^{k-n} p' (1-p')^{n-1}.
                        </me>
                        En simplifiant, on obtient :
                        <me>
                            \mathbb{P}(S_N = k) = p p' (1 - p p')^{k-1}.
                        </me>
                    </li>
                </ol>
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                On suppose que les variables aléatoires <m>X_n</m> sont à valeurs dans <m>\mathbb{N}</m>.
                <ol>
                    <li>
                        Montrer que <m>G_{S_N} = G_N \circ G_{X_1}</m> sur <m>[0, 1]</m>.
                    </li>
                    <li>
                        Montrer que, si <m>X_1</m> et <m>N</m> sont d'espérance finie, alors <m>S_N</m> est d'espérance finie et vérifie la première <em>formule de Wald</em> :
                        <me>
                            \mathbb{E}(S_N) = \mathbb{E}(X_1) \mathbb{E}(N).
                        </me>
                    </li>
                    <li>
                        Montrer que, si <m>X_1</m> et <m>N</m> possèdent un moment d'ordre 2, alors <m>S_N</m> possède aussi un moment d'ordre 2 et vérifie la seconde <em>formule de Wald</em> :
                        <me>
                            \mathbb{V}(S_N) = \mathbb{V}(X_1) \mathbb{E}(N) + (\mathbb{E}(X_1))^2 \mathbb{V}(N).
                        </me>
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                       <p>
    Pour tout <m>t \in [0, 1]</m>,
    <me>
        G_{S_N}(t) = \sum_{k \in \mathbb{N}} t^k \mathbb{P}(S_N = k) = \sum_{k \in \mathbb{N}} \sum_{n \in \mathbb{N}} t^k \mathbb{P}(\{S_N = k\} \cap \{N = n\}).
    </me>
    La famille de réels positifs <m>\big(t^k \mathbb{P}(\{S_N = k\} \cap \{N = n\})\big)_{(k, n) \in \mathbb{N}^2}</m> est donc sommable. On peut donc échanger l'ordre des sommations. Pour <m>n \in \mathbb{N}</m>, on a :
    <me>
        \sum_{k \in \mathbb{N}} t^k \mathbb{P}(\{S_N = k\} \cap \{N = n\}) = \sum_{k \in \mathbb{N}} t^k \mathbb{P}(\{S_n = k\} \cap \{N = n\})
    </me>
    <me>
        = \sum_{k \in \mathbb{N}} t^k \mathbb{P}(S_n = k) \mathbb{P}(N = n),
    </me>
    car <m>S_n</m> et <m>N</m> sont indépendantes. On en déduit :
    <me>
        \sum_{k \in \mathbb{N}} t^k \mathbb{P}(\{S_N = k\} \cap \{N = n\}) = G_{S_n}(t) \mathbb{P}(N = n)
    </me>
    <me>
        = G_{X_1}(t)^n \mathbb{P}(N = n),
    </me>
    car <m>G_{S_n}(t) = G_{X_1 + \cdots + X_n}(t) = \prod_{i=1}^n G_{X_i}(t) = G_{X_1}(t)^n</m>, par indépendance des variables <m>X_i</m>. On obtient finalement :
    <me>
        \forall t \in [0, 1], \quad G_{S_N}(t) = \sum_{n \in \mathbb{N}} (G_{X_1}(t))^n \mathbb{P}(N = n) = G_N\big(G_{X_1}(t)\big).
    </me>
</p>
                    </li>
                    <li>
                        Si <m>X_1</m> et <m>N</m> sont d'espérance finie, alors :
                        <me>
                            \mathbb{E}(S_N) = G_{S_N}'(1) = G_N'(G_{X_1}(1)) G_{X_1}'(1) = G_N'(1) G_{X_1}'(1) = \mathbb{E}(N) \mathbb{E}(X_1).
                        </me>
                    </li>
                    <li>
                        Si <m>X_1</m> et <m>N</m> possèdent un moment d'ordre 2, alors :
                        <me>
                            \mathbb{V}(S_N) = G_{S_N}''(1) + G_{S_N}'(1) - (G_{S_N}'(1))^2.
                        </me>
                        En utilisant la formule de dérivation composée, on obtient :
                        <me>
                            \mathbb{V}(S_N) = \mathbb{V}(X_1) \mathbb{E}(N) + (\mathbb{E}(X_1))^2 \mathbb{V}(N).
                        </me>
                    </li>
                </ol>
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                On revient au cas général. On suppose que <m>X_1</m> et <m>N</m> sont d'espérance finie.
                <ol>
                    <li>
                        Démontrer que la famille <m>(x \mathbb{P}(S_n = x) \mathbb{P}(N = n))_{(x, n) \in S_N(\Omega) \times N(\Omega)}</m> est sommable.
                    </li>
                    <li>
                        En déduire que <m>S_N</m> est d'espérance finie et :
                        <me>
                            \mathbb{E}(S_N) = \mathbb{E}(X_1) \mathbb{E}(N).
                        </me>
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        La famille est sommable car :
                        <me>
                            \sum_{(x, n)} |x| \mathbb{P}(S_n = x) \mathbb{P}(N = n) \leq \mathbb{E}(|X_1|) \mathbb{E}(N)\lt +\infty.
                        </me>
                    </li>
                    <li>
                        En sommant la famille, on obtient :
                        <me>
                            \mathbb{E}(S_N) = \sum_{(x, n)} x \mathbb{P}(S_n = x) \mathbb{P}(N = n) = \mathbb{E}(X_1) \mathbb{E}(N).
                        </me>
                    </li>
                </ol>
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Variables aléatoires symétriques et inégalité de Paul Lévy</title>
    <introduction>
    <p>
        Toutes les variables considérées dans cet exercice sont à valeurs dans <m>\mathbb{Z}</m>. Une variable aléatoire à valeurs dans <m>\mathbb{Z}</m> est dite symétrique si :
        <me>
            \forall n \in \mathbb{Z}, \quad \mathbb{P}(X = n) = \mathbb{P}(X = -n).
        </me>
        </p>
    </introduction>

    <task>
        <statement>
            <ol>
                <li>
                    Montrer que si <m>X</m> est symétrique, alors <m>0</m> est une médiane de <m>X</m>, c'est-à-dire :
                    <me>
                        \mathbb{P}(X\gt 0) \leq \frac{1}{2} \leq \mathbb{P}(X \geq 0).
                    </me>
                </li>
                <li>
                    Montrer que si <m>X</m> et <m>Y</m> sont deux variables aléatoires indépendantes de même loi, alors <m>X - Y</m> est symétrique.
                </li>
                <li>
                    Montrer que si <m>X</m> et <m>Y</m> sont deux variables aléatoires symétriques indépendantes, alors <m>X + Y</m> est symétrique.
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    On a :
                    <me>
                        \mathbb{P}(X\gt 0) = \sum_{n \in \mathbb{N}^*} \mathbb{P}(X = n) = \sum_{n \in \mathbb{N}^*} \mathbb{P}(X = -n) = \mathbb{P}(X\lt 0).
                    </me>
                    Comme <m>\mathbb{P}(X\gt 0) + \mathbb{P}(X\lt 0) + \mathbb{P}(X = 0) = 1</m>, on en déduit :
                    <me>
                        \mathbb{P}(X\gt 0) = \frac{1 - \mathbb{P}(X = 0)}{2} \leq \frac{1}{2}.
                    </me>
                    De même, on a :
                    <me>
                        \mathbb{P}(X \geq 0) = \mathbb{P}(X\gt 0) + \mathbb{P}(X = 0) = \frac{1 + \mathbb{P}(X = 0)}{2} \geq \frac{1}{2}.
                    </me>
                    Ainsi, <m>0</m> est une médiane de <m>X</m>.
                </li>
                <li>
                    Soit <m>X</m> et <m>Y</m> deux variables aléatoires indépendantes de même loi. Pour tout <m>n \in \mathbb{Z}</m>, on a :
                    <me>
                        \mathbb{P}(X - Y = n) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = j + n) \mathbb{P}(Y = j).
                    </me>
                    En utilisant le fait que <m>X</m> et <m>Y</m> ont même loi, on obtient :
                    <me>
                        \mathbb{P}(X - Y = -n) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = j - n) \mathbb{P}(Y = j) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = j + n) \mathbb{P}(Y = j) = \mathbb{P}(X - Y = n).
                    </me>
                    Ainsi, <m>X - Y</m> est symétrique.
                </li>
                <li>
                    Soit <m>X</m> et <m>Y</m> deux variables aléatoires symétriques indépendantes. Pour tout <m>n \in \mathbb{Z}</m>, on a :
                    <me>
                        \mathbb{P}(X + Y = n) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = j) \mathbb{P}(Y = n - j).
                    </me>
                    En utilisant la symétrie de <m>X</m> et <m>Y</m>, on obtient :
                    <me>
                        \mathbb{P}(X + Y = -n) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = -j) \mathbb{P}(Y = -n + j) = \sum_{j \in \mathbb{Z}} \mathbb{P}(X = j) \mathbb{P}(Y = n - j) = \mathbb{P}(X + Y = n).
                    </me>
                    Ainsi, <m>X + Y</m> est symétrique.
                </li>
            </ol>
        </solution>
    </task>

    <task>
        <statement>
            On considère des variables aléatoires symétriques <m>X_1, X_2, \ldots, X_n</m>, indépendantes. On se donne <m>x \geq 0</m>. Pour <m>k \in [1, n]</m>, on pose <m>S_k = \sum_{j=1}^k X_j</m> et, de plus, on note <m>\Omega_k</m> l'événement :
            <me>
                \left\{ \max_{1 \leq j \leq k-1} S_j \leq x \right\} \cap \{ S_k\gt x \} \quad \text{si} \quad k \geq 2,
            </me>
            et <m>\Omega_1 = \{ S_1\gt x \}</m>.
            <ol>
                <li>
                    Montrer que <m>X_1 + \cdots + X_n</m> est symétrique.
                </li>
                <li>
                    Montrer que, pour <m>k \in [1, n]</m>, on a :
                    <me>
                        \{ S_n - S_k \geq 0 \} \cap \Omega_k \subset \{ S_n\gt x \} \cap \Omega_k
                    </me>
                    et
                    <me>
                        \mathbb{P} \left( \{ S_n - S_k \geq 0 \} \cap \Omega_k \right) \geq \frac{1}{2} \mathbb{P}(\Omega_k).
                    </me>
                </li>
                <li>
                    Prouver que <m>\bigcup_{k=1}^n \Omega_k = \left\{ \max_{1 \leq j \leq n} S_j\gt x \right\}</m>.
                </li>
                <li>
                    En déduire l'inégalité de Paul Lévy :
                    <me>
                        \mathbb{P} \left( \max_{1 \leq j \leq n} S_j\gt x \right) \leq 2 \mathbb{P}(S_n\gt x).
                    </me>
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    Par récurrence sur <m>n</m>, en utilisant le résultat de la question 1.(c), on montre que <m>X_1 + \cdots + X_n</m> est symétrique.
                </li>
                <li>
                    <ul>
                        <li>
                            On a <m>\{ S_n - S_k \geq 0 \} \cap \Omega_k \subset \{ S_n \geq S_k \} \cap \{ S_k\gt x \} \subset \{ S_n\gt x \}</m>, d'où l'inclusion.
                        </li>
                        <li>
                            Les événements <m>\{ S_n - S_k \geq 0 \}</m> et <m>\Omega_k</m> sont indépendants car <m>S_n - S_k</m> est une fonction de <m>X_{k+1}, \ldots, X_n</m>, tandis que <m>\Omega_k</m> est une fonction de <m>X_1, \ldots, X_k</m>. Ainsi :
                            <me>
                                \mathbb{P} \left( \{ S_n - S_k \geq 0 \} \cap \Omega_k \right) = \mathbb{P}(S_n - S_k \geq 0) \mathbb{P}(\Omega_k).
                            </me>
                            Comme <m>S_n - S_k</m> est symétrique, on a <m>\mathbb{P}(S_n - S_k \geq 0) \geq \frac{1}{2}</m>, d'où le résultat.
                        </li>
                    </ul>
                </li>
                <li>
                    L'événement <m>\left\{ \max_{1 \leq j \leq n} S_j\gt x \right\}</m> est réalisé si, et seulement s'il existe un indice <m>k \in [1, n]</m> tel que <m>S_k\gt x</m> et <m>S_j \leq x</m> pour tout <m>j\lt k</m>. Cela correspond exactement à la définition de <m>\Omega_k</m>.
                </li>
                <li>
                    En utilisant les résultats précédents, on a :
                    <me>
                        \mathbb{P} \left( \max_{1 \leq j \leq n} S_j\gt x \right) = \sum_{k=1}^n \mathbb{P}(\Omega_k) \leq 2 \sum_{k=1}^n \mathbb{P} \left( \{ S_n\gt x \} \cap \Omega_k \right) = 2 \mathbb{P}(S_n\gt x).
                    </me>
                </li>
            </ol>
        </solution>
    </task>
</exercise>


<exercise>
    <title>Succès consécutifs</title>
    <introduction>
        <p>
            On considère une suite d'épreuves de Bernoulli indépendantes. À chaque épreuve, la probabilité de succès est <m>p \in ]0, 1[</m>. On se donne un entier <m>r</m> strictement positif. Pour <m>n \in \N^{*}</m>, on note <m>\Pi_{n}</m> la probabilité qu'au cours des <m>n</m> premières épreuves, on ait obtenu <m>r</m> succès consécutifs (au moins une fois).
        </p>
        </introduction>
        <task>
            <p>
                Calculer <m>\Pi_{0}</m>, <m>\Pi_{1}</m>, ..., <m>\Pi_{r}</m>.
            </p>
        </task>
        <task>
            <p>
                Montrer que, pour <m>n \geqslant r</m>, on a <m>\Pi_{n+1} = \Pi_{n} + (1 - \Pi_{n-r}) p^{r} (1 - p)</m>.
            </p>
        </task>
        <task>
            <p>
                Montrer que la suite <m>(\Pi_{n})_{n \in \N}</m> est convergente. Calculer sa limite.
            </p>
        </task>
        <task>
            <p>
                Déduire de la question 1 que l'on peut définir une variable aléatoire <m>T</m> égale au temps d'attente de <m>r</m> succès consécutifs. On définira <m>(T = k)</m> comme l'événement « on a obtenu des succès aux épreuves de rang <m>k - r + 1</m>, <m>k - r + 2</m>, ..., <m>k</m> sans jamais avoir obtenu <m>r</m> succès consécutifs auparavant ».
            </p>
        </task>
        <task>
            <p>
                Montrer que :
                <me>
                    \EE(T) = \frac{1 - p^{r}}{(1 - p) p^{r}}.
                </me>
            </p>
        </task>
</exercise>


<exercise>
    <title>Marche aléatoire dans <m>\mathbb{Z}</m> : premier retour à l'origine</title>
    <introduction>
        <p>
            Soit <m>(X_{n})_{n \in \N^{*}}</m> une suite de variables aléatoires, sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \PP</m>, indépendantes et de même loi définie par :
            <me>
                \Pr{X_{n} = 1} = p \quad \text{et} \quad \Pr{X_{n} = -1} = 1 - p,
            </me>
            où <m>p \in [0, 1]</m>. On pose <m>S_{0} = 0</m> et, pour tout <m>n \in \N^{*}</m>, <m>S_{n} = \sum_{k=1}^{n} X_{k}</m>. La suite <m>(S_{n})</m> est appelée marche aléatoire dans <m>\mathbb{Z}</m>.
        </p>
    </introduction>
        <task>
            <p>
                Déterminer <m>u_{n} = \Pr{S_{n} = 0}</m> pour tout <m>n \in \N</m>.
            </p>
        </task>
        <task>
            <p>
                On note <m>f(x)</m> la somme de la série entière <m>\sum u_{n} x^{n}</m>. Montrer que :
                <me>
                    \forall x \in ]-1, 1[ \quad f(x) = \frac{1}{\sqrt{1 - 4 p (1 - p) x^{2}}}.
                </me>
            </p>
        </task>
        <task>
            <p>
                Pour tout entier naturel non nul <m>k</m>, on note <m>A_{k}</m> l'événement « le mobile retourne pour la première fois à l'origine au bout de <m>k</m> déplacements », c'est-à-dire :
                <me>
                    A_{k} = (S_{k} = 0) \cap \left(\bigcap_{i=1}^{k-1} (S_{i} \neq 0)\right).
                </me>
                On pose <m>v_{k} = \Pr{A_{k}}</m> pour tout <m>k \geqslant 1</m> et <m>v_{0} = 0</m>.
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que, pour tout entier naturel <m>n</m> non nul, on a :
                        <me>
                            (S_{n} = 0) = \sum_{k=1}^{n} \Pr{(S_{n} = 0) \cap A_{k}}.
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire que, pour tout entier naturel non nul <m>n</m>, on a :
                        <me>
                            u_{n} = \sum_{k=0}^{n} u_{n - k} v_{k}.
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>



<exercise>
    <title>Inégalité de Kolmogorov</title>
    <introduction>
        <p>
            Soit <m>X_{1}, \ldots, X_{n}</m> des variables aléatoires réelles discrètes de l'espace probabilisé <m>(\Omega, \mathcal{A}, \PP</m>, indépendantes, ayant un moment d'ordre 2, centrées, ainsi que <m>a \in \R_{+}^{*}</m>. On pose, pour tout <m>i \in \llbracket 1, n \rrbracket</m> :
            <me>
                S_{i} = X_{1} + \cdots + X_{i}, \quad B_{i} = \left\{\left|S_{1}\right| \lt a\right\} \cap \ldots \cap \left\{\left|S_{i-1}\right| \lt a\right\} \cap \left\{\left|S_{i}\right| \geqslant a\right\}.
            </me>
        </p>
    </introduction>
        <task>
            <p>
                Montrer que, pour <m>i \in \llbracket 1, n \rrbracket</m>, les variables <m>S_{i} \mathbf{1}_{B_{i}}</m> et <m>S_{n} - S_{i}</m> sont indépendantes. En déduire que :
                <me>
                    \EE(S_{n}^{2} \mathbf{1}_{B_{i}}) = \EE(S_{i}^{2} \mathbf{1}_{B_{i}}) + \EE((S_{n} - S_{i})^{2} \mathbf{1}_{B_{i}}) \geqslant a^{2} \Pr{B_{i}}.
                </me>
            </p>
        </task>
        <task>
            <p>
                On pose <m>C = \left\{\sup \left(\left|S_{1}\right|, \left|S_{2}\right|, \ldots, \left|S_{n}\right|\right) \geqslant a\right\}</m>. Montrer que <m>\Pr{C} = \sum_{i=1}^{n} \Pr{B_{i}}</m>.
            </p>
        </task>
        <task>
            <p>
                En déduire l'inégalité de Kolmogorov :
                <me>
                    \Pr{\sup \left(\left|S_{1}\right|, \left|S_{2}\right|, \ldots, \left|S_{n}\right|\right) \geqslant a}
                    \leqslant \frac{\VV(S_{n})}{a^{2}}.
                </me>
            </p>
        </task>
</exercise>



<exercise>
    <title>Inégalité de Le Cam</title>
    <introduction>
        <p>
            L'objet de l'exercice est d'étudier l'approximation de la loi binomiale par la loi de Poisson. Toutes les variables aléatoires considérées sont définies sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \PP</m> et sont à valeurs dans <m>\N</m>.
        </p>
    </introduction>
        <task>
            <p>
                Soit <m>X</m> et <m>Y</m> deux telles variables aléatoires. Pour tout <m>k \in \N</m>, on pose <m>p_{k} = \Pr{X = k}</m> et <m>q_{k} = \Pr{Y = k}</m>. On définit la distance entre <m>X</m> et <m>Y</m> par :
                <me>
                    \mathrm{d}(X, Y) = \frac{1}{2} \sum_{k=0}^{+\infty} \left|p_{k} - q_{k}\right|.
                </me>
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que pour toute partie <m>A</m> de <m>\N</m>, on a :
                        <me>
                            |\Pr{X \in A} - \Pr{Y \in A}| \leqslant \mathrm{d}(X, Y).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        Démontrer la formule :
                        <me>
                            \mathrm{d}(X, Y) = 1 - \sum_{k=0}^{+\infty} \min(p_{k}, q_{k}).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire :
                        <me>
                            \mathrm{d}(X, Y) \leqslant \Pr{X \neq Y}.
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>



<exercise>
    <title>Convergence presque sûre</title>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \in \mathbb{N}}</m> une suite de variables aléatoires réelles et <m>X</m> une variable aléatoire réelle définies sur <m>(\Omega, \mathcal{A}, \mathbb{P})</m>. On pose :
            <me>
                B = \left\{ \omega \in \Omega \mid \lim_{n \to +\infty} X_n(\omega) = X(\omega) \right\}.
            </me>
            On dit que la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m> si <m>\mathbb{P}(B) = 1</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que l'on a <m>\mathbb{P}(B) = \lim_{k \to +\infty} \mathbb{P}(C_k)</m>, où :
                <me>
                    C_k = \bigcap_{n \in \mathbb{N}} \bigcup_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\}.
                </me>
            </p>
        </statement>
        <solution>
            <p>
                On a :
                <me>
                    B = \left\{ \omega \in \Omega \mid \forall k \in \mathbb{N}^*, \exists n \in \mathbb{N}, \forall p \geq n, |X_p(\omega) - X(\omega)| \leq \frac{1}{k} \right\}.
                </me>
                En d'autres termes, <m>B</m> est l'ensemble des <m>\omega \in \Omega</m> pour lesquels la suite <m>(X_n(\omega))</m> converge vers <m>X(\omega)</m>. On peut réécrire <m>B</m> comme :
                <me>
                    B = \bigcap_{k \in \mathbb{N}^*} \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\}.
                </me>
                Par continuité décroissante de la probabilité, on a :
                <me>
                    \mathbb{P}(B) = \lim_{k \to +\infty} \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\} \right) = \lim_{k \to +\infty} \mathbb{P}(C_k).
                </me>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                On suppose que :
                <me>
                    \forall \varepsilon \gt 0, \quad \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Montrer que la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </statement>
        <solution>
            <p>
                Par hypothèse, pour tout <m>\varepsilon \gt 0</m>, on a :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Cela signifie que, pour tout <m>\varepsilon \gt 0</m>, l'ensemble des <m>\omega \in \Omega</m> pour lesquels <m>|X_p(\omega) - X(\omega)| \gt \varepsilon</m> pour une infinité de <m>p</m> est de probabilité nulle. Par conséquent, pour presque tout <m>\omega \in \Omega</m>, la suite <m>(X_n(\omega))</m> converge vers <m>X(\omega)</m>, c'est-à-dire que <m>\mathbb{P}(B) = 1</m>.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Montrer que si la série de terme général <m>\mathbb{P}(|X_n - X| \gt \varepsilon)</m> converge pour tout <m>\varepsilon \gt 0</m>, alors la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </statement>
        <solution>
            <p>
                Si la série de terme général <m>\mathbb{P}(|X_n - X| \gt \varepsilon)</m> converge pour tout <m>\varepsilon \gt 0</m>, alors, d'après le lemme de Borel-Cantelli, on a :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                En effet, le lemme de Borel-Cantelli affirme que si <m>\sum_{n=1}^{+\infty} \mathbb{P}(A_n) \lt +\infty</m>, alors <m>\mathbb{P}\left( \limsup_{n \to +\infty} A_n \right) = 0</m>. Appliqué à <m>A_n = \{ |X_n - X| \gt \varepsilon \}</m>, cela donne :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Par conséquent, la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Fonction génératrice des moments</title>
    <introduction>
    <p>
        Soit <m>X</m> une variable aléatoire discrète, pas presque sûrement constante, sur l'espace probabilisé <m>(\Omega, \mathcal{A}, \mathbb{P})</m>. On pose, pour <m>t \in \mathbb{R}</m>, 
        <me>
            L_X(t) = \mathbb{E}\left(e^{tX}\right),
        </me>
        <m>L_X</m> est appelée la <em>fonction génératrice des moments</em> de la variable aléatoire <m>X</m>. On suppose qu'il existe un intervalle <m>]\alpha,\beta[</m> contenant <m>0</m> tel que <m>L_X(t)\lt +\infty</m> pour tout <m>t \in ]\alpha,\beta[</m>.
        </p>
    </introduction>

    <!-- Première tâche -->
    <task>
        <statement>
            <ol>
                <li>
                    Soit <m>a\lt b</m> deux réels tels que <m>[a, b] \subset ]\alpha,\beta[</m>. On considère <m>\delta\gt 0</m> tel que <m>[a - \delta, b + \delta] \subset ]\alpha,\beta[</m>. Soit <m>k \in \mathbb{N}</m>. Montrer qu'il existe <m>C\gt 0</m> tel que :
                    <me>
                        \forall t \in [a, b], \quad \forall u \in \mathbb{R}, \quad |u|^k e^{tu} \leq C \left(e^{(a - \delta)u} + e^{(b + \delta)u}\right).
                    </me>
                    En déduire que <m>X^k e^{tX}</m> est d'espérance finie pour tout <m>t \in ]\alpha,\beta[</m>.
                </li>
                <li>
                    Montrer que <m>L_X</m> est de classe <m>C^\infty</m> sur <m>]\alpha,\beta[</m> et vérifie :
                    <me>
                        \forall t \in ]\alpha,\beta[, \quad \forall k \in \mathbb{N}, \quad L_X^{(k)}(t) = \mathbb{E}\left(X^k e^{tX}\right).
                    </me>
                    En déduire, pour tout <m>k \in \mathbb{N}</m>, une expression du moment d'ordre <m>k</m> de <m>X</m>. On note <m>m</m> l'espérance de <m>X</m>.
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    Pour <m>t \in [a, b]</m> et <m>u \in \mathbb{R}</m>, on a :
                    <me>
                        |u|^k e^{tu} \leq |u|^k e^{b|u|} \leq C \left(e^{(a - \delta)u} + e^{(b + \delta)u}\right),
                    </me>
                    où <m>C</m> est une constante positive. Comme <m>e^{(a - \delta)X}</m> et <m>e^{(b + \delta)X}</m> ont une espérance finie, il en est de même pour <m>X^k e^{tX}</m>.
                </li>
                <li>
                    La fonction <m>L_X</m> est dérivable sur <m>]\alpha,\beta[</m> et sa dérivée est donnée par :
                    <me>
                        L_X'(t) = \mathbb{E}\left(X e^{tX}\right).
                    </me>
                    Par récurrence, on montre que <m>L_X</m> est de classe <m>C^\infty</m> et que :
                    <me>
                        L_X^{(k)}(t) = \mathbb{E}\left(X^k e^{tX}\right).
                    </me>
                    En particulier, pour <m>t = 0</m>, on a :
                    <me>
                        L_X^{(k)}(0) = \mathbb{E}(X^k).
                    </me>
                </li>
            </ol>
        </solution>
    </task>

    <!-- Deuxième tâche -->
    <task>
        <statement>
            <ol>
                <li>
                    On pose, pour tout <m>t \in ]\alpha,\beta[</m>, <m>\Psi_X(t) = \ln L_X(t)</m>. Montrer que <m>\Psi_X</m> est strictement convexe.
                </li>
                <li>
                    On note <m>I = \Psi_X'\left(]\alpha,\beta[\right)</m> et on pose <m>g(c) = \max_{t \in ]\alpha,\beta[} \left(ct - \Psi_X(t)\right)</m>, pour tout <m>c \in I</m>. Montrer que <m>m \in I</m>. Calculer <m>g(m)</m> ; montrer que <m>g(c)\gt 0</m> si <m>c \neq m</m>.
                </li>
                <li>
                    Montrer que :
                    <me>
                        g(c) = \begin{cases}
                            \max_{t \in (\alpha, 0)} \left(ct - \Psi_X(t)\right) \amp\text{si } c\lt m, \\
                            \max_{t \in (0, \beta)} \left(ct - \Psi_X(t)\right) \amp\text{si } c\gt m.
                        \end{cases}
                    </me>
                </li>
                <li>
                    En déduire les inégalités de Chernov :
                    <me>
                        \mathbb{P}(X \leq c) \leq e^{-g(c)} \quad \text{si } c\lt m,
                    </me>
                    <me>
                        \mathbb{P}(X \geq c) \leq e^{-g(c)} \quad \text{si } c\gt m.
                    </me>
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    La fonction <m>\Psi_X</m> est strictement convexe car sa dérivée seconde est strictement positive :
                    <me>
                        \Psi_X''(t) = \frac{L_X''(t) L_X(t) - (L_X'(t))^2}{L_X(t)^2}\gt 0.
                    </me>
                </li>
                <li>
                    On a <m>m = \mathbb{E}(X) = L_X'(0) = \Psi_X'(0)</m>, donc <m>m \in I</m>. De plus, <m>g(m) = 0</m> car <m>\Psi_X(0) = 0</m>. Si <m>c \neq m</m>, alors <m>g(c)\gt 0</m> par convexité de <m>\Psi_X</m>.
                </li>
                <li>
                    La fonction <m>t \mapsto ct - \Psi_X(t)</m> atteint son maximum en un point <m>t_c</m> tel que <m>\Psi_X'(t_c) = c</m>. Si <m>c\lt m</m>, alors <m>t_c \in (\alpha, 0)</m> ; si <m>c\gt m</m>, alors <m>t_c \in (0, \beta)</m>.
                </li>
                <li>
                    En utilisant l'inégalité de Markov, on a :
                    <me>
                        \mathbb{P}(X \leq c) = \mathbb{P}\left(e^{t_c X} \geq e^{t_c c}\right) \leq \frac{\mathbb{E}\left(e^{t_c X}\right)}{e^{t_c c}} = e^{-g(c)}.
                    </me>
                    De même, pour <m>c\gt m</m>, on a :
                    <me>
                        \mathbb{P}(X \geq c) \leq e^{-g(c)}.
                    </me>
                </li>
            </ol>
        </solution>
    </task>
</exercise>


<exercise>
    <title>Théorème de Weierstrass</title>
    <introduction>
        <p>
            Soit <m>f</m> une fonction continue de <m>[0, 1]</m> dans <m>\R</m>. Soit <m>x \in [0, 1]</m>. On considère une suite <m>(X_{n})_{n \geqslant 1}</m> de variables de Bernoulli de paramètre <m>x</m>, indépendantes, sur le même espace probabilisé. Pour <m>n \geqslant 1</m>, on pose <m>Y_{n} = \frac1n(X_{1} + \cdots + X_{n})</m>.
        </p>
        </introduction>
        <!-- <task>
            <p>
                Soit <m>\varepsilon\gt0</m>. Par uniforme continuité de <m>f</m> sur <m>[0, 1]</m>, il existe <m>\eta\gt0</m> tel que :
                <me>
                    \forall(t, u) \in [0, 1]^{2} \quad |t - u| \leqslant \eta \Longrightarrow |f(t) - f(u)| \leqslant \varepsilon.
                </me>
            </p>
        </task> -->
        <task>
            <p>
                Montrer que :
                <me>
                    \forall(t, u) \in [0, 1]^{2} \quad |f(t) - f(u)| \leqslant \frac{2 \Vert f \Vert _{\infty} (t - u)^{2}}{\eta^{2}} + \varepsilon.
                </me>
            </p>
        </task>
        <task>
            <p>
                Montrer que :
                <me>
                    \left|\EE(f(Y_{n})) - f(x)\right| \leqslant \frac{2 \Vert f \Vert _{\infty} \VV(Y_{n})}{\eta^{2}} + \varepsilon \leqslant \frac{2 \Vert f \Vert _{\infty}}{n \eta^{2}} + \varepsilon.
                </me>
            </p>
        </task>
        <task >
            <statement>
                <p>
                   En déduire que la suite de fonctions polynomiales <m>(B_n(f))_n</m> définie par 
                   <me>
                    \forall t\in[0,1],\;
                    B_n(f)(t)\sum_{k=0}^nf(k/n)\binom{n}{k}t^k(1-t)^{n-k}
                   </me>
                   converge uniformément vers <m>f</m> sur <m>[0,1]</m>.
                </p>
            </statement>
        </task>
</exercise>

<exercise>
    <title>File d'attente</title>
    <introduction>
        <p>
            Soit <m>n</m> un entier supérieur ou égal à 2. On considère une file d'attente avec un guichet et <m>n</m> clients qui attendent. Chaque minute, un guichet se libère. Le guichetier choisit alors le client qu'il appelle selon le processus aléatoire suivant :
            <ul>
                <li>avec probabilité 2, il appelle le client en première position dans la file,</li>
                <li>sinon, il choisit de manière équiprobable parmi les <m>n - 1</m> autres clients.</li>
            </ul>
            Enfin, un nouveau client arrive dans la file et se place en dernière position (de telle sorte qu'il y a toujours exactement <m>n</m> clients qui attendent). Pour tout <m>k \in \llbracket 1, n \rrbracket</m>, on note <m>T_{k}</m> le temps d'attente d'un client qui se trouve en position <m>k</m> dans la file.
        </p>
        </introduction>
        <task>
            <p>
                Quelle est la loi de <m>T_{1}</m> ? Donner son espérance, sa variance.
            </p>
        </task>
        <task>
            <p>
                Montrer que, pour tout <m>k \in \llbracket 1, n \rrbracket</m>, la variable <m>T_{k}</m> est d'espérance finie.
            </p>
        </task>
        <task>
            <p>
                Écrire une relation entre <m>\EE(T_{k})</m> et <m>\EE(T_{k-1})</m> pour tout <m>k \geqslant 2</m>. En déduire une expression de <m>\EE(T_{k})</m> en fonction de <m>k</m> et <m>n</m>. On pourra considérer la suite <m>((n + k - 2) \EE(T_{k}))_{1 \leqslant k \leqslant n}</m>.
            </p>
        </task>
        <task>
            <p>
                Comparer les caractéristiques de cette file d'attente et d'une file d'attente « classique » (premier arrivé, premier servi).
            </p>
        </task>
</exercise>

<exercise>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \geq 1}</m> une suite de variables aléatoires réelles discrètes, toutes de même loi, et <m>N</m> une variable aléatoire à valeurs dans <m>\mathbb{N}</m>. On suppose que <m>N</m> et les variables <m>X_n</m>, pour <m>n \in \mathbb{N}^*</m>, forment une suite de variables aléatoires indépendantes. On pose :
            <me>
                \forall n \in \mathbb{N}^*, \quad S_n = \sum_{k=1}^n X_k \quad \text{et} \quad S_0 = 0.
            </me>
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que <m>S_N</m> est une variable aléatoire.
            </p>
        </statement>
        <solution>
            <p>
                On a <m>S_N(\Omega) \subset \{0\} \cup \bigcup_{n \in \mathbb{N}^*} S_n(\Omega)</m>. Les <m>S_n(\Omega)</m> étant au plus dénombrables, il en est de même de leur union dénombrable, et a fortiori <m>S_N(\Omega)</m> est au plus dénombrable. De plus :
                <me>
                    \forall x \in S_N(\Omega), \quad \{S_N = x\} = \bigcup_{n \in \mathbb{N}} \{S_n = x\} \cap \{N = n\},
                </me>
                donc <m>\{S_N = x\}</m>, union dénombrable d'événements, est un événement et <m>S_N</m> est une variable aléatoire.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>S_N</m>, lorsque les <m>X_k</m> suivent la loi de Bernoulli de paramètre <m>p</m> et <m>N</m> la loi de Poisson de paramètre <m>\lambda</m>.
            </p>
        </statement>
        <solution>
            <p>
                On a <m>S_N(\Omega) = \mathbb{N}</m> et d'après la formule des probabilités totales, pour tout <m>k \in \mathbb{N}</m> :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=0}^{+\infty} \mathbb{P}(S_N = k \mid N = n) \mathbb{P}(N = n).
                </me>
                On remarque que :
                <me>
                    \mathbb{P}(S_N = k \mid N = n) = \mathbb{P}(S_n = k \mid N = n) = \mathbb{P}(S_n = k),
                </me>
                la dernière égalité résultant de l'indépendance des variables <m>X_n</m> par rapport à <m>N</m>. Pour <m>n \geq 1</m>, <m>S_n</m> est une somme de <m>n</m> variables de Bernoulli, indépendantes, de même paramètre <m>p</m> ; elle suit donc la loi binomiale de paramètre <m>(n, p)</m>. On a donc :
                <me>
                    \mathbb{P}(S_n = k) = 
                    \begin{cases} 
                        \binom{n}{k} p^k (1 - p)^{n - k}  \amp   \text{si } k \leq n \\
                        0  \amp   \text{si } k \gt n.
                    \end{cases}
                </me>
                On remarque que cette formule reste vérifiée si <m>n = 0</m>, car alors <m>S_n</m> prend la valeur 0, avec la probabilité 1. On en déduit :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=k}^{+\infty} \binom{n}{k} p^k (1 - p)^{n - k} e^{-\lambda} \frac{\lambda^n}{n!}.
                </me>
                En simplifiant, on obtient :
                <me>
                    \mathbb{P}(S_N = k) = e^{-\lambda p} \frac{(\lambda p)^k}{k!}.
                </me>
                La variable aléatoire <m>S_N</m> suit donc la loi de Poisson de paramètre <m>\lambda p</m>.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>S_N</m> lorsque les <m>X_k</m> suivent la loi géométrique de paramètre <m>p</m> et <m>N</m> la loi géométrique de paramètre <m>p'</m>.
            </p>
        </statement>
        <solution>
            <p>
                On procède comme dans la question précédente. On a <m>S_N(\Omega) = \mathbb{N}^*</m> et, pour tout <m>n \in \mathbb{N}^*</m> :
                <me>
                    \mathbb{P}(S_N = k \mid N = n) = \mathbb{P}(S_n = k \mid N = n) = \mathbb{P}(S_n = k).
                </me>
                Comme les variables <m>X_k</m> sont à valeurs dans <m>\mathbb{N}^*</m>, il est clair que <m>\mathbb{P}(S_n = k) = 0</m> si <m>k \lt  n</m>. Supposons <m>k \geq n</m>. On peut alors écrire :
                <me>
                    \{S_n = k\} = \bigcup_{(i_1, i_2, \ldots, i_n) \in J_k} \{X_1 = i_1\} \cap \{X_2 = i_2\} \cap \ldots \cap \{X_n = i_n\},
                </me>
                où <m>J_k</m> est l'ensemble des <m>n</m>-listes d'entiers strictement positifs <m>(i_1, i_2, \ldots, i_n)</m> tels que <m>i_1 + i_2 + \cdots + i_n = k</m>. Le cardinal de <m>J_k</m> est égal au nombre de <m>(n-1)</m>-listes <m>(j_1, j_2, \ldots, j_{n-1})</m> d'entiers tels que <m>1 \leq j_1 \lt  j_2 \lt  \ldots \lt  j_{n-1} \leq k - 1</m>, car à <m>(i_1, i_2, \ldots, i_n)</m>, on peut associer bijectivement :
                <me>
                    (j_1, j_2, \ldots, j_{n-1}) = (i_1, i_1 + i_2, \ldots, i_1 + i_2 + \cdots + i_{n-1}).
                </me>
                On a donc <m>\text{card}(J_k) = \binom{k-1}{n-1}</m>.
                D'autre part, pour <m>(i_1, i_2, \ldots, i_n) \in J_k</m>, on a, par indépendance des variables aléatoires <m>X_i</m> :
                <me>
                    \mathbb{P}(\{X_1 = i_1\} \cap \{X_2 = i_2\} \cap \ldots \cap \{X_n = i_n\}) = \prod_{j=1}^n \mathbb{P}(X_j = i_j) = \prod_{j=1}^n p(1 - p)^{i_j - 1} = p^n (1 - p)^{k - n}.
                </me>
                On obtient ainsi <m>\mathbb{P}(S_n = k) = \binom{k-1}{n-1} p^n (1 - p)^{k - n}</m>. On en déduit, pour <m>k \in \mathbb{N}^*</m> :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=1}^{+\infty} \mathbb{P}(S_N = k \mid N = n) \mathbb{P}(N = n) = \sum_{n=1}^k \binom{k-1}{n-1} p^n (1 - p)^{k - n} p' (1 - p')^{n - 1}.
                </me>
                En simplifiant, on obtient :
                <me>
                    \mathbb{P}(S_N = k) = p p' (1 - p p')^{k - 1}.
                </me>
                La variable <m>S_N</m> suit donc la loi géométrique de paramètre <m>p p'</m>.
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <introduction>
    <p>
        On considère une suite d'épreuves de Bernoulli indépendantes. À chaque épreuve, la probabilité de succès est <m>p \in ]0,1[</m>. On se donne un entier <m>r</m> strictement positif. Pour <m>n \in \mathbb{N}^*</m>, on note <m>\Pi_n</m> la probabilité qu'au cours des <m>n</m> premières épreuves, on ait obtenu <m>r</m> succès consécutifs (au moins une fois).
        </p>
    </introduction>

    <task>
        <statement>
            <ol>
                <li>Calculer <m>\Pi_0</m>, <m>\Pi_1</m>, ..., <m>\Pi_r</m>.</li>
                <li>Montrer que, pour <m>n \geq r</m>, on a <m>\Pi_{n+1} = \Pi_n + (1 - \Pi_{n-r}) p^r (1 - p)</m>.</li>
                <li>Montrer que la suite <m>(\Pi_n)_{n \in \mathbb{N}}</m> est convergente. Calculer sa limite.</li>
            </ol>
        </statement>
        <solution>
            <p>
            <ol>
                <li>
                    On a <m>\Pi_0 = \Pi_1 = \ldots = \Pi_{r-1} = 0</m> et <m>\Pi_r = p^r</m>.
                </li>
                <li>
                    Pour <m>n \geq 1</m>, on note <m>S_n</m> l'événement « la <m>n</m>-ième épreuve est un succès » et <m>A_n</m> l'événement « au cours de <m>n</m> premiers tirages, on a obtenus <m>r</m> succès consécutifs ». On a clairement <m>A_n \subset A_{n+1}</m> et donc <m>\Pi_{n+1} - \Pi_n = \mathbb{P}(A_{n+1} \setminus A_n)</m>. L'événement <m>A_{n+1} \setminus A_n</m> est réalisé si on obtient <m>r</m> succès consécutifs pour la première fois entre le <m>(n-r+2)</m>-ième et le <m>(n+1)</m>-ième tirage, ce qui impose que la <m>(n-r+1)</m>-ième épreuve soit un échec et qu'on n'ait pas avant obtenu <m>r</m> succès consécutifs. On a donc :
                    <me>
                        A_{n+1} \setminus A_n = A_{n-r} \cap S_{n-r+1} \cap S_{n-r+2} \cap \cdots \cap S_{n+1}
                    </me>
                    et par indépendance des épreuves :
                    <me>
                        \Pi_{n+1} - \Pi_n = (1 - \Pi_{n-r}) (1 - p) p^r.
                    </me>
                </li>
                <li>
                    La suite <m>(\Pi_n)_n</m> est croissante et majorée par 1 donc convergente. On note <m>L</m> sa limite. Par passage à la limite dans la relation précédente, on obtient <m>L - L = (1 - L)(1 - p)p^r</m>, et donc <m>L = 1</m>, puisque <m>p \in [0,1]</m>.
                </li>
            </ol>
        </p>
        </solution>

    </task>

    <task>
        <statement>
            <ol>
                <li>Déduire de la question 1 que l'on peut définir une variable aléatoire <m>T</m> égale au temps d'attente de <m>r</m> succès consécutifs. On définira <m>\{T = k\}</m> comme l'événement « on a obtenu des succès aux épreuves de rang <m>k-r+1</m>, <m>k-r+2</m>, ..., <m>k</m> sans jamais avoir obtenu <m>r</m> succès consécutifs auparavant ».</li>
                <li>Montrer en utilisant le résultat de l'exercice 18 de la page 911 que :
                    <me>
                        \mathbb{E}(T) = \frac{1 - p^r}{(1 - p)p^r}.
                    </me>
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    Comme la suite <m>(A_n)</m> est croissante, on a :
                    <me>
                        \mathbb{P} \left( \bigcup_{n \in \mathbb{N}^*} A_n \right) = \lim_{n \to +\infty} \Pi_n = 1.
                    </me>
                    On obtient de manière presque sûre une suite de <m>r</m> succès consécutifs au bout d'un nombre fini d'épreuves. Sur un ensemble de probabilité 1, on peut définir l'application <m>T</m>.
                    On a, par définition <m>\{T = k\} = A_{k+1} \setminus A_k</m>, donc <m>\{T = k\}</m> est un événement et <m>T</m> une variable aléatoire à valeurs dans <m>\mathbb{N}^*</m>.
                </li>
                <li>
                    Pour tout <m>k \in \mathbb{N}</m>, on a <m>\{T \gt k\} = A_k</m> et donc <m>\mathbb{P}(T \gt k) = 1 - \Pi_k</m>. D'après la question 1, on a pour tout <m>k \geq 0</m> :
                    <me>
                        1 - \Pi_k = \frac{\Pi_{k+r+1} - \Pi_{k+r}}{(1 - p)p^r}.
                    </me>
                    D'après l'exercice 18 de la page 911, on en déduit :
                    <me>
                        \mathbb{E}(T) = \sum_{k=0}^{+\infty} \frac{\Pi_{k+r+1} - \Pi_{k+r}}{(1 - p)p^r} = \frac{\lim_{n \to +\infty} \Pi_n - \Pi_r}{(1 - p)p^r} = \frac{1 - p^r}{(1 - p)p^r}.
                    </me>
                </li>
            </ol>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Exercice 16.21</title>
    <task>
        <statement>
            <p>
                Soit <m> X_1, X_2, \ldots, X_n </m> des variables aléatoires indépendantes et identiquement distribuées à valeurs dans <m> \mathbb{Z} </m>. On suppose que <m> X_1 </m> est d'espérance finie. Montrer qu'il existe une variable aléatoire <m> Y_1 </m> telle que :
                <ol>
                    <li><m> Y_1 </m> est une fonction de <m> X_1 </m>.</li>
                    <li>Pour toute fonction <m> f </m> bornée, on a :
                        <me>
                        \mathbb{E}(X_2 f(X_1)) = \mathbb{E}(Y_1 f(X_1)).
                        </me>
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                Si <m> f </m> est bornée, alors <m> f(X_1) </m> est également bornée. Comme <m> X_2 </m> est d'espérance finie, la variable aléatoire <m> X_2 f(X_1) </m> est également d'espérance finie. En appliquant le théorème de transfert à la variable <m> (X_1, X_2) </m> et à la fonction <m> (x, y) \mapsto y f(x) </m>, on obtient :
                <me>
                \mathbb{E}(X_2 f(X_1)) = \sum_{(m, n) \in \mathbb{Z}^2} n f(m) \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}).
                </me>
            </p>
            <p>
                La famille <m> (n f(m) \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}))_{(m, n) \in \mathbb{Z}^2} </m> est sommable, donc on peut écrire :
                <me>
                \mathbb{E}(X_2 f(X_1)) = \sum_{m \in \mathbb{Z}} f(m) \sum_{n \in \mathbb{Z}} n \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}).
                </me>
            </p>
            <p>
                Si <m> \mathbb{P}(X_1 = m) \neq 0 </m>, alors on peut écrire :
                <me>
                \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}) = \mathbb{P}(X_1 = m) \mathbb{P}(X_2 = n \mid X_1 = m),
                </me>
                et donc :
                <me>
                \sum_{n \in \mathbb{Z}} n \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}) = \mathbb{P}(X_1 = m) \sum_{n \in \mathbb{Z}} n \mathbb{P}(X_2 = n \mid X_1 = m).
                </me>
                La dernière somme est l'espérance conditionnelle de <m> X_2 </m> sachant <m> X_1 = m </m>, notée <m> \mathbb{E}(X_2 \mid X_1 = m) </m>.
            </p>
            <p>
                Si <m> \mathbb{P}(X_1 = m) = 0 </m>, alors <m> \sum_{n \in \mathbb{Z}} n \mathbb{P}(\{X_1 = m\} \cap \{X_2 = n\}) = 0 </m>. On définit donc :
                <me>
                h(m) = 
                \begin{cases}
                    \mathbb{E}(X_2 \mid X_1 = m) \amp \text{si } \mathbb{P}(X_1 = m) \neq 0, \\
                    0 \amp \text{sinon}.
                \end{cases}
                </me>
                On obtient alors :
                <me>
                \mathbb{E}(X_2 f(X_1)) = \sum_{m \in \mathbb{Z}} \mathbb{P}(X_1 = m) f(m) h(m).
                </me>
                Par le théorème de transfert, on a :
                <me>
                \mathbb{E}(X_2 f(X_1)) = \mathbb{E}(h(X_1) f(X_1)).
                </me>
                Ainsi, <m> Y_1 = h(X_1) </m> satisfait les conditions demandées.
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Montrer que <m> Y_1 </m> est unique presque sûrement.
            </p>
        </statement>
        <solution>
            <p>
                Supposons qu'il existe une autre variable aléatoire <m> Y_1' = h'(X_1) </m> satisfaisant les mêmes conditions. Pour tout <m> m \in \mathbb{Z} </m> tel que <m> \mathbb{P}(X_1 = m) \neq 0 </m>, on a :
                <me>
                h'(m) = \mathbb{E}(X_2 \mid X_1 = m) = h(m).
                </me>
                Pour <m> m \in \mathbb{Z} </m> tel que <m> \mathbb{P}(X_1 = m) = 0 </m>, <m> h'(m) </m> peut être quelconque, mais cela n'affecte pas l'espérance. Ainsi, <m> Y_1 </m> et <m> Y_1' </m> coïncident presque sûrement.
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Montrer que pour toute fonction <m> g </m> bornée, on a :
                <me>
                \mathbb{E}(X_2 g(X_1) \mid X_1) = g(X_1) \mathbb{E}(X_2 \mid X_1).
                </me>
            </p>
        </statement>
        <solution>
            <p>
                Par définition de l'espérance conditionnelle, il existe une fonction <m> k </m> définie sur <m> \mathbb{Z} </m> telle que :
                <me>
                \mathbb{E}(X_2 g(X_1) \mid X_1) = k(X_1).
                </me>
                D'après les étapes précédentes, on a :
                <me>
                k(m) = \mathbb{E}(X_2 g(X_1) \mid X_1 = m) = g(m) h(m).
                </me>
                Ainsi, on a :
                <me>
                \mathbb{E}(X_2 g(X_1) \mid X_1) = g(X_1) h(X_1) = g(X_1) \mathbb{E}(X_2 \mid X_1).
                </me>
            </p>
        </solution>
    </task>
</exercise>



 </chapter>
