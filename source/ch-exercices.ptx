<?xml version="1.0" encoding="UTF-8"?>





<chapter xml:id="ch-exercices">
    <title>Exercices</title>
    



<exercise><title>Caractérisation de la loi géometrique décalée</title>


    <introduction>
    <p>
        On considère <m> X </m> et <m> Y </m> deux variables aléatoires sur <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>, à valeurs dans <m> \mathbb{N} </m>, indépendantes, de même loi. On pose <m> D = X - Y </m> et <m> I = \min(X, Y) </m>.
    </p>
    </introduction>

    <task>
        <statement>
            On suppose que pour tout <m> k </m> dans <m> \mathbb{N} </m>, <m> \mathbb{P}(X = k) = pq^k </m>, où <m> p \in [0, 1[ </m> et <m> q = 1 - p </m>.
            <ol>
                <li>Déterminer la loi conjointe de <m> (D, I) </m>.</li>
                <li>Déterminer les lois marginales de <m> D </m> et <m> I </m>. Vérifier que <m> D </m> et <m> I </m> sont indépendantes.</li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    <p>
                    On a <m> D(\Omega) = \mathbb{Z} </m> et <m> I(\Omega) = \mathbb{N} </m>.
                    <ul>
                        <li><p>Si <m> k \geq 0 </m>, alors on a :
                            <me>
                            \{D = k\} \cap \{I = \ell\} = \{X - Y = k\} \cap \{Y = \ell\} = \{X = k + \ell\} \cap \{Y = \ell\}
                            </me>
                            et donc, par indépendance de <m> X </m> et <m> Y </m> :
                            <me>
                            \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(\{X = k + \ell\} \cap \{Y = \ell\}) = pq^{k + \ell} pq^\ell = p^2 q^{k + 2\ell}.
                            </me>
                            </p>
                        </li>
                        <li>
                            <p> Si <m> k \lt 0 </m>, alors on a :
                            <me>
                            \{D = k\} \cap \{I = \ell\} = \{X - Y = k\} \cap \{X = \ell\} = \{X = \ell\} \cap \{Y = -k + \ell\}
                            </me>
                            et donc, par indépendance de <m> X </m> et <m> Y </m> :
                            <me>
                            \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(\{X = \ell\} \cap \{Y = -k + \ell\}) = pq^\ell pq^{-k + \ell} = p^2 q^{-k + 2\ell}.
                            </me>
                            </p>
                        </li>
                    </ul>
                    </p>
                    <p>
                    Dans tous les cas, on trouve <m> \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = p^2 q^{|k| + 2\ell} </m>.
                    </p>
                </li>
                <li>
                    <ul>
                        <li>Pour <m> k \in \mathbb{Z} </m>, on a :
                            <me>
                            \mathbb{P}(D = k) = \sum_{\ell=0}^{+\infty} \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \sum_{\ell=0}^{+\infty} p^2 q^{|k| + 2\ell} = \frac{p^2 q^{|k|}}{1 - q^2} = \frac{p q^{|k|}}{1 + q}.
                            </me>
                        </li>
                        <li>Pour <m> \ell \in \mathbb{N} </m>, on a :
                            <me>
                            \mathbb{P}(I = \ell) = \sum_{k=-\infty}^{+\infty} \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \sum_{k=-\infty}^{+\infty} p^2 q^{|k| + 2\ell} = p^2 q^{2\ell} \left( \frac{2}{1 - q} - 1 \right) = p q^{2\ell} (1 + q).
                            </me>
                        </li>
                    </ul>
                    <p>
                    On vérifie que <m> D </m> et <m> I </m> sont indépendantes car :
                    <me>
                    \mathbb{P}(\{D = k\} \cap \{I = \ell\}) = \mathbb{P}(D = k) \mathbb{P}(I = \ell).
                    </me>
                    </p>
                </li>
            </ol>
        </solution>
    </task>

    <task>
        <statement>
            <p>
            On suppose que les variables <m> D </m> et <m> I </m> sont indépendantes et que <m> \mathbb{P}(X = n) \neq 0 </m> pour tout <m> n \in \mathbb{N} </m>. Montrer qu'il existe <m> p \in [0, 1[ </m>, tel que, pour tout <m> k \in \mathbb{N} </m>,
            <me>
            \mathbb{P}(X = k) = p q^k.
            </me>
            </p>
        </statement>
        <solution>
            <p>
            Comme précédemment, on a :
            <me>
            \forall k \in \mathbb{N}, \quad \{D = k\} \cap \{I = \ell\} = \{X = k + \ell\} \cap \{Y = \ell\}.
            </me>
            Par indépendance de <m> X </m> et <m> Y </m> d'une part, de <m> D </m> et <m> I </m> d'autre part, on en déduit :
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = \ell) = \mathbb{P}(X = k + \ell) \mathbb{P}(Y = \ell) = \mathbb{P}(X = k + \ell) \mathbb{P}(X = \ell) \neq 0,
            </me>
            par hypothèse. On a en particulier, pour tout <m> k \in \mathbb{N} </m> :
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = 0) = \mathbb{P}(X = k) \mathbb{P}(X = 0),
            </me>
            <me>
            \mathbb{P}(D = k) \mathbb{P}(I = 1) = \mathbb{P}(X = k + 1) \mathbb{P}(X = 1).
            </me>
            En divisant les égalités, on obtient :
            <me>
            \frac{\mathbb{P}(X = k + 1)}{\mathbb{P}(X = k)} = \frac{\mathbb{P}(I = 1) \mathbb{P}(X = 0)}{\mathbb{P}(I = 0) \mathbb{P}(X = 1)}.
            </me>
            Ce rapport est indépendant de <m> k </m> et strictement positif. On le note <m> q </m>. La suite <m> (\mathbb{P}(X = k)) </m> est géométrique de raison <m> q </m>. Pour tout <m> k \in \mathbb{N} </m>, on a <m> \mathbb{P}(X = k) = \mathbb{P}(X = 0) q^k </m>. La série <m> \sum \mathbb{P}(X = k) </m> converge et a pour somme 1 donc <m> q \lt 1 </m> et <m> \mathbb{P}(X = 0) = 1 - q </m>. En posant <m> p = 1 - q </m>, on a le résultat voulu.
            </p>
        </solution>
    </task>
</exercise>

    <exercise>
        <title>Une caractérisation de la loi de Poisson</title>
    

        <introduction>
            <p>
                On considère une variable aléatoire discrète <m>N</m> sur l'espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m> telle que <m>N(\Omega)=\N</m> et <m>\Pr(N=n) \neq 0</m> pour tout <m>n \in \N</m>.
                Si la variable aléatoire <m>N</m> prend la valeur <m>n</m>, on procède à une succession de <m>n</m> épreuves de Bernoulli indépendantes de paramètre <m>p \in] 0,1[</m>.
                On note <m>S</m> et <m>E</m> les variables aléatoires représentant respectivement le nombre de succès et d'échecs dans ces <m>n</m> épreuves.
            </p>
        </introduction>


        <task>
            <p>
                Montrer que si <m>N</m> suit une loi de Poisson de paramètre <m>\lambda>0</m>, les variables <m>S</m> et <m>E</m> suivent aussi des lois de Poisson dont on déterminera les paramètres.
                Montrer que les variables <m>E</m> et <m>S</m> sont indépendantes.
            </p>
        </task>


        <task>
            <p>
                Montrer réciproquement que si <m>S</m> et <m>E</m> sont indépendantes, alors <m>N</m> suit une loi de Poisson.
                Pour cela, on montrera :
            </p>

            <ul>
                <li>
                    <p>
                        qu'il existe deux suites <m>\left(u_{n}\right)_{n \in \N}</m> et <m>\left(v_{n}\right)_{n \in \N}</m> telles que :
                        <me>
                            \forall(m, n) \in \N^{2} \quad(m+n) ! \Pr(N=m+n)=u_{m} v_{n}
                        </me>
                    </p>
                </li>

                <li>
                    <p>
                        que les suites <m>\left(u_{n}\right)_{n \in \N}</m> et <m>\left(v_{n}\right)_{n \in \N}</m> sont géométriques.
                    </p>
                </li>
            </ul>
        </task>
    </exercise>

    <exercise>
        <title>Lancer de pièce jusqu'au premier pile</title>

            <introduction>
                <p>
                    On lance une pièce de monnaie jusqu'à l'obtention du premier pile, la probabilité d'obtenir pile étant <m>p \in] 0,1[</m>.
                    Soit <m>N</m> la variable aléatoire représentant le nombre de lancers nécessaires.
                    Si <m>N=n</m>, on relance ensuite <m>n</m> fois la pièce et on appelle <m>X</m> la variable aléatoire représentant le nombre de piles obtenu.
                </p>
            </introduction>


            <task>
                <p>
                    Déterminer la loi de <m>N</m>, celle du couple <m>(N, X)</m>, puis la loi de <m>X</m>.
                </p>
            </task>


            <task>
                <p>
                    Montrer que <m>X</m> a même loi que le produit de deux variables indépendantes <m>Y</m> et <m>Z</m> telles que <m>Y</m> suive une loi de Bernoulli et <m>Z</m> une loi géométrique de même paramètre.
                </p>
            </task>


            <task>
                <p>
                    En déduire l'espérance et la variance de <m>X</m>.
                </p>
            </task>
    </exercise>

    <exercise>
        <title>Loi sur les nombres premiers</title>
        <introduction>
            <p>
                Soit <m>\mathcal{P}</m> l'ensemble des nombres premiers.
                Soit <m>X</m> une variable aléatoire à valeurs dans <m>\N^{*}</m> dont la loi est définie par :
                <me>
                    \forall n \in \N^{*} \quad \Pr(X=n)=\frac{n^{-s}}{\zeta(s)}
                </me>
            </p>
        </introduction>

            <task>
                <p>
                    Justifier qu'on définit bien ainsi la loi d'une variable aléatoire.
                </p>
            </task>


            <task>
                <p>
                    Pour tout <m>n \in \N^{*}</m>, on considère <m>A_{n}</m> : « <m>n</m> divise <m>X</m> ».
                    Montrer que <m>\left(A_{p}\right)_{p \in \mathcal{P}}</m> est une famille d'événements indépendants.
                    En déduire une preuve de :
                    <me>
                        \prod_{p \in \mathcal{P}}\left(1-\frac{1}{p^{s}}\right)=\frac{1}{\zeta(s)}
                    </me>
                </p>
            </task>


            <task>
                <p>
                    Montrer que la probabilité qu'aucun carré différent de 1 ne divise <m>X</m> vaut <m>\frac{1}{\zeta(2 s)}</m>.
                </p>
            </task>
    </exercise>

    <exercise>
        <title>Taux de panne</title>

        <introduction>
            <p>
                Soit <m>X</m> une variable aléatoire discrète à valeurs dans <m>\N^{*}</m> vérifiant :
                <me>
                    \forall n \in \N^{*} \quad \Pr(X \geqslant n)>0
                </me>
                <m>X</m> représente le moment où un mécanisme tombe en panne. C'est à dire le numéro de l'instance de son cycle de fonctionnement où il tombe en panne. En principe, sous l'effet de l'usure, plus la durée de son fonctionnement est grande plus la probabilité que le mécanisme tombe en panne augmente. 
                </p> 
                <p>que On appelle taux de panne associé à <m>X</m> la suite réelle <m>\left(x_{n}\right)_{n \in \N^{*}}</m> définie par :
                <me>
                    \forall n \in \N^{*} \quad x_{n}=\Pr(X=n \mid X \geqslant n)
                </me>
                <m>x_n</m> est la probabilité pour que le mécanisme tombe en panne à l'instant <m>n</m> sachant qu'il a fonctionné jusqu'à cet instant. 
            </p>
            </introduction>


            <task>
                
                <statement>
                    <p>
                        Exprimer <m>p_{n}=\Pr(X=n)</m> en fonction des <m>x_{k}</m>.
                    </p>
                </statement>
                <hint>
                    <p>
                        Éviter de diviser par <m>x_n</m>. Exprimer <m>\Pr(X\geq n)</m> comme un produit de facteurs <m>(1-x_k)</m>.
                    </p>
                </hint>
                <answer>
                    <p>
                        <me>
                            \forall n \in \N^{*} \quad p_{n}=x_{n}\prod_{k=1}^{n-1}(1-x_{k})
                        </me> 
                    </p>
                </answer>

                <solution>
                    <p>
                        On a <m>(X=n)\subset (X\geqslant n)</m> donc <m>\Pr(X=n)=\Pr(X=n,X\geq n)</m>.
                        Ce qui donne
                        <men xml:id="eq-tauxpanne">
                            x_n=\Pr(X=n\mid X\geq n)=\frac{\Pr(X=n)}{\Pr(X\geq n)}
                        </men>
                        On en déduit que 
                        <me>
                            1-x_n=\frac{\Pr(X\geq n)-\Pr(X=n)}{\Pr(X\geq n)}=\frac{\Pr(X\geq n+1)}{\Pr(X\geq n)}
                        </me>
                        Ce qui donne par télescopage 
                        <me>
                            \prod_{k=1}^{n-1}(1-x_k)=\frac{\Pr(X\geq n)}{\Pr(X\geq 1)}=\Pr(X\geq n)
                        </me>
                        La relation <xref ref="eq-tauxpanne"/> signifie que <m>p_n=x_n\Pr(X\geq n)</m> donc finalement 
                        <men xml:id="eqn-pnexpr">
                            p_n=x_n\prod_{k=1}^{n-1}(1-x_k)
                        </men>
                        
                        
                        
                        











                        
                    </p>
                </solution>
            </task>


            <task><title>Caractérisation du taux de panne</title>
                <statement>
                    <p>
                        <ol marker="1.">
                            <li>
                                <p>
                                    Montrer que <m>0 \leqslant x_{n} \lt 1</m> pour tout <m>n \in \N^{*}</m> et que la série de terme général <m>x_{n}</m> diverge.
                                </p>
                            </li>

                            <li>
                                <p>
                                    Réciproquement, soit <m>\left(x_{n}\right)_{n \in \N^{*}}</m> une suite à valeur dans <m>[0,1[</m> telle que la série de terme général <m>x_{n}</m> diverge.
                                    Montrer qu'il existe une variable aléatoire dont le taux de panne est la suite <m>\left(x_{n}\right)</m>.
                                </p>
                            </li>
                        </ol>
                    </p>
                </statement>
                <hint>
                    <p>
                        On rappelle que pour une suite <m>(p_n)_n</m> de réels positifs sommable et de somme <m>1</m>, il existe une variable aléatoire <m>X</m> telle que <m>\Pr(X=n)=p_n</m> pour tout <m>n</m>.   
                    </p>
                </hint>

                <solution>
                    <ol marker="1.">
                        <li>
                            <p>
                                Soit <m>n\in\N^*</m> et supposons que <m>x_n=1</m>.
                                Alors <m>p_n=\Pr(X\geq n)</m>, ou encore <m>\Pr(X=n)=\Pr(X\geq n)</m>.
                                Ce qui implique que <m>\Pr(X\geq n+1)=0</m> contredisant l'hypothèse faite dans l'énoncé.
                                Alors <m>x_n\lt1</m>.
                            </p>

                            <p>
                                Ensuite <m>P(X\geq n+1)</m> est le reste de la série convergente <m>\sum p_k</m> donc il converge vers <m>0</m>. Ce qui implique que 
                                <me>
                                \sum_{k=1}^{n}\ln(1-x_n)=\ln\bigl(\Pr(X\geq n)\bigr)\longrightarrow -\infty
                                </me>
                                La série <m>\sum x_n</m> est nécessairement divergente car dans le cas contraire <m>(x_n)_n</m> convergerait vers <m>0</m> et on aurait donc <m>-\ln(1-x_n)\sim x_n</m> ce qui impliquerait que la série <m>\sum \ln(1-x_n)</m> est convergente.  
                            </p>
                        </li>

                        <li>
                            <p> Soit <m>\sum x_n</m> une série divergente à termes dans <m>[0,1[</m> et posons <m>v_1=1</m> pour tout <m>n\geq 2</m>
                                <me>
                                    v_n=\prod_{k=1}^{n-1}(1-x_k) \qtext{et} p_n=x_nv_n
                                </me>
                            Avec ces relations on a 
                                <me>
                                    v_n-v_{n+1}=v_nx_n=p_n
                                </me>
                            On peut ensuite écrire 
                            <me>
                                \ln v_n=\sum_{k=1}^{n-1}\ln(1-x_n)\leqslant -\sum_{k=1}^{n-1} x_k
                            </me>
                            Puisque la série de réels positif<m>\sum x_n</m> est divergente positive sa suite des sommes partielles tend vers <m>+\infty</m> et on a donc 
                            <m>\ln v_n\longrightarrow-\infty</m>. Par suite <m>v_n\longrightarrow 0</m>. Puisque <m>p_n=v_n-v_{n+1}</m> alors la série <m>\sum p_n</m> est convergente de somme <m>v_1=1</m>.
                            </p>
                            <p>
                                Il existe donc une VADR <m>X</m> tel que <m>P(X=n)=p_n</m> pour tout <m>n\in\N^*</m>. 
                            </p>
                        </li>
                    </ol>
                </solution>
            </task>


            <task>
            <statement>
                <p>
                    Montrer que la variable <m>X</m> suit une loi géométrique si, et seulement si, son taux de panne est constant.
                </p>
            </statement>
            <solution>
                <p>
                    On suppose que <m>X</m> suit une loi géométrique de paramètre <m>p</m>. Alors pour tout <m>n\in\N^*</m> on a
                    <m>P(X=n)=p(1-p)^{n-1}</m>. Donc 
                    <me>P(X=n+1)=\sum_{k=n+1}p(1-p)^{k-1}=(1-p)^n</me>
                    Par suite 
                    <me>\Pr(X=n\mid X\geq n)=\frac{\Pr(X=n)}{\Pr(X\geq n)}=\frac{p(1-p)^{n-1}}{(1-p)^{n-1}}=p</me>. 
                    Donc le taux de panne est constant.

                    Réciproquement, on suppose que le taux de panne est constant de valeur <m>p</m>. Alors pour tout <m>n\in\N^*</m> on a
                    <me>P(X=n)=x_n\prod_{k=1}^{n-1}(1-x_k)=p(1-p)^{n-1}</me>. 
                    
                    Donc <m>X</m> suit une loi géométrique de paramètre <m>p</m>.
                </p>
                <p> Noter que cela signifie que le taux de panne est constant si et seulement si les événements «le mécanisme tombe en panne à l'instant <m>n</m>» sont mutuellement indépendants et ont tous la même probabilité. Il n'y a aucun effet d'usure.
                </p>
            </solution>
            </task>
    </exercise>


   <exercise>
    <title>Maximums et minimums provisoires</title>
    <introduction>
        <p>
            Soit <m>n \in \mathbb{N}^*</m>. On désigne par <m>\Omega</m> l'ensemble des permutations de <m>[1, n]</m>. On munit <m>\Omega</m> de la probabilité uniforme. Pour <m>\sigma \in \Omega</m> et <m>i \in [1, n]</m>, on dit que <m>\sigma(i)</m> est un <em>maximum provisoire</em> (resp. <em>minimum provisoire</em>) de <m>\sigma</m> si :
            <me>
                \sigma(i) = \max(\sigma(1), \sigma(2), \ldots, \sigma(i)) \quad \text{(resp. } \sigma(i) = \min(\sigma(1), \sigma(2), \ldots, \sigma(i))\text{)}.
            </me>
            On désigne par <m>X_n</m> (resp. <m>Y_n</m>) les variables aléatoires représentant le nombre de maximums (resp. minimums) provisoires des permutations de <m>[1, n]</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que les variables <m>X_n</m> et <m>Y_n</m> ont même loi.
            </p>
        </statement>
        <solution>
            <p>
                On observe qu'en <m>1</m>, il y a toujours un maximum et un minimum provisoire, et donc que <m>X_n</m> et <m>Y_n</m> sont à valeurs dans <m>[1, n]</m>.
                L'application <m>f : \Omega \to \Omega</m> qui à la permutation <m>\sigma</m> associe la permutation <m>\sigma' : k \mapsto n + 1 - \sigma(k)</m> est clairement bijective. Pour <m>i \in [1, n]</m>, <m>\sigma(i)</m> est un maximum provisoire de <m>\sigma</m> si, et seulement si, <m>\sigma'(i)</m> est un minimum provisoire de <m>\sigma</m>. En effet :
                <me>
                    \sigma(i) = \max(\sigma(1), \ldots, \sigma(i)) \quad \text{équivaut à} \quad n + 1 - \sigma'(i) = \max(n + 1 - \sigma'(1), \ldots, n + 1 - \sigma'(i)),
                </me>
                ce qui équivaut à :
                <me>
                    \sigma'(i) = \min(\sigma'(1), \ldots, \sigma'(i)).
                </me>
                On en déduit que, pour tout <m>k \in [1, n]</m>,
                <me>
                    \sigma \in \{X_n = k\} \quad \text{si et seulement si} \quad \sigma' \in \{Y_n = k\}.
                </me>
                Comme <m>f</m> est bijective, on a <m>\text{card}(\{X_n = k\}) = \text{card}(\{Y_n = k\})</m>, et donc <m>\mathbb{P}(X_n = k) = \mathbb{P}(Y_n = k)</m>, car <m>\Omega</m> est muni de la probabilité uniforme.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>X_3</m>, son espérance et sa variance.
            </p>
        </statement>
        <solution>
            <p>
                <ul>
                    <li>
                        La permutation <m>\sigma</m> est dans <m>\{X_3 = 1\}</m> si, et seulement si, <m>\sigma(1) = 3</m>. On en déduit <m>\mathbb{P}(X_3 = 1) = \frac{1}{3}</m>.
                    </li>
                    <li>
                        On a <m>\{X_3 = 3\} = \{\text{Id}_{[1,3]}\}</m>. On en déduit <m>\mathbb{P}(X_3 = 3) = \frac{1}{6}</m>.
                    </li>
                    <li>
                        Enfin, <m>\mathbb{P}(X_3 = 2) = 1 - \frac{1}{3} - \frac{1}{6} = \frac{1}{2}</m>.
                    </li>
                </ul>
                On obtient :
                <me>
                    \mathbb{E}(X_3) = \frac{11}{6} \quad \text{et} \quad \mathbb{V}(X_3) = \frac{17}{36}.
                </me>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi du couple <m>(X_3, Y_3)</m> et sa covariance.
            </p>
        </statement>
        <solution>
            <p>
                Pour <m>(k, \ell) \in [1, 3]^2</m>, on a <m>\mathbb{P}(X_3 = k, Y_3 = \ell) = 0</m> si <m>k + \ell \geq 4</m>. En effet, sauf pour <m>i = 1</m>, <m>\sigma(i)</m> ne peut pas être à la fois un maximum provisoire et un minimum provisoire. On obtient :
                <me>
                    \begin{array}{|c|c|c|c|}
                        \hline
                        X  \amp   Y  \amp   1  \amp   2  \amp   3 \\
                        \hline
                        1  \amp   0  \amp   \frac{1}{6}  \amp   \frac{1}{6} \\
                        \hline
                        2  \amp   \frac{1}{6}  \amp   \frac{1}{3}  \amp   0 \\
                        \hline
                        3  \amp   \frac{1}{6}  \amp   0  \amp   0 \\
                        \hline
                    \end{array}
                </me>
                On trouve :
                <me>
                    \mathbb{E}(X_3 Y_3) = 3 \quad \text{et} \quad \text{Cov}(X_3, Y_3) = -\frac{13}{36}.
                </me>
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Formule du crible</title>
    <task>
        <statement>
            <p>
                Soit <m> A_1, A_2, \ldots, A_n </m> des événements d'un espace probabilisé <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>. Montrer que :
                <me>
                1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}).
                </me>
                En déduire la formule du crible :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \sum_{k=1}^n \left( (-1)^{k-1} \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right) \right).
                </me>
            </p>
        </statement>
        <solution>
            <p>
                Pour montrer l'égalité <m> 1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}) </m>, on observe que :
                <ul>
                    <li>
                        Si <m> \omega \in \bigcup_{i=1}^n A_i </m>, alors il existe au moins un <m> i </m> tel que <m> \omega \in A_i </m>, donc <m> 1_{A_i}(\omega) = 1 </m>. Ainsi, <m> \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 0 </m>, et donc <m> 1 - \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 1 </m>.
                    </li>
                    <li>
                        Si <m> \omega \notin \bigcup_{i=1}^n A_i </m>, alors <m> \omega \notin A_i </m> pour tout <m> i </m>, donc <m> 1_{A_i}(\omega) = 0 </m>. Ainsi, <m> \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 1 </m>, et donc <m> 1 - \prod_{i=1}^n (1 - 1_{A_i}(\omega)) = 0 </m>.
                    </li>
                </ul>
                On en déduit que <m> 1_{\bigcup_{i=1}^n A_i} = 1 - \prod_{i=1}^n (1 - 1_{A_i}) </m>.

                Pour obtenir la formule du crible, on utilise l'espérance de cette égalité :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \mathbb{E}\left( 1_{\bigcup_{i=1}^n A_i} \right) = 1 - \mathbb{E}\left( \prod_{i=1}^n (1 - 1_{A_i}) \right).
                </me>
                En développant le produit, on obtient :
                <me>
                \prod_{i=1}^n (1 - 1_{A_i}) = \sum_{k=0}^n (-1)^k \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \prod_{i \in I} 1_{A_i}.
                </me>
                En prenant l'espérance, on a :
                <me>
                \mathbb{E}\left( \prod_{i=1}^n (1 - 1_{A_i}) \right) = \sum_{k=0}^n (-1)^k \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right).
                </me>
                En substituant dans l'expression précédente, on obtient la formule du crible :
                <me>
                \mathbb{P}\left( \bigcup_{i=1}^n A_i \right) = \sum_{k=1}^n (-1)^{k-1} \sum_{\substack{I \subset [1, n] \\ \text{card } I = k}} \mathbb{P}\left( \bigcap_{i \in I} A_i \right).
                </me>
            </p>
        </solution>
    </task>
    <task>
        <statement>
            <p>
                Soit <m> n \in \mathbb{N}^* </m> et <m> (X_k)_{k \in \mathbb{N}^*} </m> une suite de variables indépendantes d'un espace probabilisé <m> (\Omega, \mathcal{A}, \mathbb{P}) </m>, suivant toutes la loi uniforme sur <m>[1, n]</m>. On note <m> X </m> la variable aléatoire égale au nombre de tirages nécessaires pour obtenir tous les numéros entre 1 et <m> n </m> au moins une fois (et à <m> +\infty </m> si on n'obtient jamais les <m> n </m> numéros). Pour <m> j \in [1, n] </m> et <m> m \in \mathbb{N} </m>, on note <m> B_{j,m} </m> l'événement : « au bout de <m> m </m> tirages, le numéro <m> j </m> n'est pas encore apparu ».
            </p>
            <ol>
                <li>
                    Calculer <m> \mathbb{P}(B_{j_1,m} \cap B_{j_2,m} \cap \cdots \cap B_{j_k,m}) </m>, où <m> j_1, j_2, \ldots, j_k </m> sont des indices distincts compris entre 1 et <m> n </m>.
                </li>
                <li>
                    En déduire que <m> \mathbb{P}(X\gt m) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m </m>. Calculer <m> \lim_{m \to +\infty} \mathbb{P}(X\gt m) </m>. Interpréter.
                </li>
                <li>
                    Montrer que <m> \mathbb{E}(X) = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k} </m>, en utilisant l'exercice 18 de la page 911.
                </li>
                <li>
                    Montrer que <m> \mathbb{E}(X) = n \left( 1 + \frac{1}{2} + \cdots + \frac{1}{n} \right) </m>. En déduire un équivalent de <m> \mathbb{E}(X) </m> quand <m> n </m> tend vers <m> +\infty </m>.
                </li>
            </ol>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        Pour <m> j_1, j_2, \ldots, j_k </m> des indices distincts compris entre 1 et <m> n </m>, on a :
                        <me>
                        \mathbb{P}(B_{j_1,m} \cap B_{j_2,m} \cap \cdots \cap B_{j_k,m}) = \left( \frac{n - k}{n} \right)^m.
                        </me>
                        En effet, chaque tirage a une probabilité <m> \frac{n - k}{n} </m> de ne pas tomber sur l'un des <m> k </m> numéros <m> j_1, j_2, \ldots, j_k </m>.
                    </li>
                    <li>
                        En appliquant la formule du crible, on a :
                        <me>
                        \mathbb{P}(X\gt m) = \mathbb{P}\left( \bigcup_{j=1}^n B_{j,m} \right) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m.
                        </me>
                        La limite <m> \lim_{m \to +\infty} \mathbb{P}(X\gt m) = 0 </m> car <m> \left( \frac{n - k}{n} \right)^m </m> tend vers 0 pour tout <m> k \geq 1 </m>. Cela signifie qu'il est presque sûr que tous les numéros seront obtenus en un nombre fini de tirages.
                    </li>
                    <li>
                        En utilisant l'exercice 18 de la page 911, on a :
                        <me>
                        \mathbb{E}(X) = \sum_{m=0}^{+\infty} \mathbb{P}(X\gt m) = \sum_{m=0}^{+\infty} \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \left( \frac{n - k}{n} \right)^m.
                        </me>
                        En intervertissant les sommes, on obtient :
                        <me>
                        \mathbb{E}(X) = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \sum_{m=0}^{+\infty} \left( \frac{n - k}{n} \right)^m = \sum_{k=1}^n (-1)^{k-1} \binom{n}{k} \frac{1}{1 - \frac{n - k}{n}} = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k}.
                        </me>
                    </li>
                    <li>
                        On a :
                        <me>
                        \mathbb{E}(X) = n \sum_{k=1}^n (-1)^{k-1} \frac{\binom{n}{k}}{k} = n \left( 1 + \frac{1}{2} + \cdots + \frac{1}{n} \right).
                        </me>
                        Quand <m> n </m> tend vers <m> +\infty </m>, on a <m> \mathbb{E}(X) \sim n \ln n </m>.
                    </li>
                </ol>
            </p>
        </solution>
    </task>
</exercise>



<exercise>
    <title>Variables aléatoires uniformes et Poisson</title>
    <introduction>
        <p>
            Soient un entier <m>n \geqslant 1</m> et une suite <m>(U_{k})_{k \in \N^{*}}</m> de variables aléatoires indépendantes et de même loi uniforme sur <m>\llbracket 1, n \rrbracket</m>. Pour tout <m>i \in \llbracket 1, n \rrbracket</m>, on définit :
            <me>
                X_{i}^{(0)} = 0 \quad \text{et} \quad X_{i}^{(m)} = \card\{k \in \llbracket 1, m \rrbracket \mid U_{k} = i\} \quad \forall m \geqslant 1.
            </me>
        </p>
        </introduction>
        <task>
            <p>
                Quelle est la loi de <m>X_{i}^{(m)}</m> pour <m>i \in \llbracket 1, n \rrbracket</m> et <m>m \geqslant 1</m> ?
            </p>
        </task>
        <task>
            <p>
                Soit <m>m \geqslant 1</m> et <m>(i, j) \in \llbracket 1, n \rrbracket^{2}</m> avec <m>i \neq j</m>. Calculer la covariance des variables aléatoires <m>X_{i}^{(m)}</m> et <m>X_{j}^{(m)}</m>. Sont-elles indépendantes ?
            </p>
        </task>
        <task>
            <p>
                Soit <m>\lambda\gt0</m> et <m>N</m> une variable aléatoire suivant une loi de Poisson de paramètre <m>\lambda</m>, indépendante des variables <m>U_{k}</m>. On pose :
                <me>
                    \forall i \in \llbracket 1, n \rrbracket \quad Y_{i} = X_{i}^{(N)}.
                </me>
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Déterminer, en fonction de <m>\lambda</m> et <m>n</m>, la loi de <m>Y_{i}</m> pour tout <m>i \in \llbracket 1, n \rrbracket</m>.
                    </p>
                </li>
                <li>
                    <p>
                        Déterminer la loi conjointe de <m>(Y_{1}, \ldots, Y_{n})</m>.
                    </p>
                </li>
            </ol>
        </task>
</exercise>


<exercise xml:id="centrale_2015">
    <title>Centrale 2015</title>
    
    <task>
        <statement>
            <p>Soit <m>(\Omega, A, P)</m> un espace probabilisé et <m>(E_n)_{n \in \mathbb{N}}</m> 
            une suite d’événements.</p>
            <p>On suppose que la série suivante converge :</p>
            <me>
                \sum_{n=0}^{+\infty} P(E_n) \lt +\infty.
            </me>
        </statement>
        <solution>
            <p>La convergence de cette série joue un rôle essentiel dans l'analyse des probabilités des événements.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>On note <m>1_X</m> la fonction indicatrice d’un ensemble <m>X</m>. 
            Soit <m>Z = \sum_{n=0}^{+\infty} 1_{E_n}</m> (avec la convention <m>Z = +\infty</m> 
            si la série diverge). Prouver que <m>Z</m> est une variable aléatoire discrète.</p>
        </statement>
        <solution>
            <p>On montre que <m>Z</m> prend ses valeurs dans <m>\mathbb{N} \cup \{+\infty\}</m>, 
            qui est un ensemble dénombrable. Pour <m>n \in \mathbb{N}</m>, on a :</p>
            <me>
                \{Z = n\} = \bigcup_{I \in P_n(\mathbb{N})} 
                \left( \bigcap_{k \in I} E_k \cap \bigcap_{k \in \mathbb{N} \setminus I} E_k^c \right).
            </me>
            <p>Chaque ensemble à l'intérieur de cette union est un événement, donc <m>\{Z = n\}</m> 
            est un événement. De même, on montre que <m>\{Z = +\infty\}</m> est un événement, donc 
            <m>Z</m> est bien une variable aléatoire discrète.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>Soit <m>F = \{\omega \in \Omega \mid \omega</m> appartient à un nombre fini de 
            <m>E_n</m> (pour <m>n \in \mathbb{N}</m>)}. Prouver que <m>F</m> est un événement 
            et que <m>P(F) = 1</m>.</p>
        </statement>
        <solution>
            <p>La série <m>\sum P(E_n)</m> converge, donc d'après le premier lemme de <em>Borel-Cantelli</em>,</p>
            <me>
                P\left( \bigcap_{n \in \mathbb{N}} \bigcup_{p \geq n} E_p \right) = 0.
            </me>
            <p>Cela signifie que <m>P(Z = +\infty) = 0</m>. Donc, presque sûrement, 
            <m>\omega</m> appartient à un nombre fini d'événements <m>E_n</m>, ce qui montre que 
            <m>F</m> est un événement et que <m>P(F) = 1</m>.</p>
        </solution>
    </task>

    <task>
        <statement>
            <p>Prouver que <m>Z</m> est d'espérance finie.</p>
        </statement>
        <solution>
            <p>On définit <m>Z_n = \sum_{i=0}^{n} 1_{E_i}</m>. Alors :</p>
            <me>
                E(Z_n) = \sum_{i=0}^{n} P(E_i).
            </me>
            <p>Par passage à la limite, et en utilisant la convergence de la série :</p>
            <me>
                E(Z) = \sum_{i=0}^{+\infty} P(E_i).
            </me>
            <p>Ainsi, <m>Z</m> est bien d'espérance finie.</p>
        </solution>
    </task>
</exercise>




<exercise>
    <title>Marche aléatoire dans <m>\mathbb{Z}</m></title>
    <introduction>
        <p>
            Soit <m>(X_{n})_{n \in \N^{*}}</m> une suite de variables aléatoires, sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m>, indépendantes et de même loi définie par :
            <me>
                \Pr(X_{n} = 1) = p \quad \text{et} \quad \Pr(X_{n} = -1) = 1 - p,
            </me>
            où <m>p \in [0, 1]</m>. On pose <m>S_{0} = 0</m> et, pour tout <m>n \in \N^{*}</m>, <m>S_{n} = \sum_{k=1}^{n} X_{k}</m>. La suite <m>(S_{n})</m> est appelée marche aléatoire dans <m>\mathbb{Z}</m>.
        </p>
        </introduction>
        <task>
            <p>
                Déterminer <m>u_{n} = \Pr(S_{n} = 0)</m> pour tout <m>n \in \N</m>.
            </p>
        </task>
        <task>
            <p>
                On note <m>f(x)</m> la somme de la série entière <m>\sum u_{n} x^{n}</m>. Montrer que :
                <me>
                    \forall x \in ]-1, 1[ \quad f(x) = \frac{1}{\sqrt{1 - 4 p (1 - p) x^{2}}}.
                </me>
            </p>
        </task>
        <task>
            <p>
                Pour tout entier naturel non nul <m>k</m>, on note <m>A_{k}</m> l'événement « le mobile retourne pour la première fois à l'origine au bout de <m>k</m> déplacements », c'est-à-dire :
                <me>
                    A_{k} = (S_{k} = 0) \cap \left(\bigcap_{i=1}^{k-1} (S_{i} \neq 0)\right).
                </me>
                On pose <m>v_{k} = \Pr(A_{k})</m> pour tout <m>k \geqslant 1</m> et <m>v_{0} = 0</m>.
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que, pour tout entier naturel <m>n</m> non nul, on a :
                        <me>
                            (S_{n} = 0) = \sum_{k=1}^{n} \Pr((S_{n} = 0) \cap A_{k}).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire que, pour tout entier naturel non nul <m>n</m>, on a :
                        <me>
                            u_{n} = \sum_{k=0}^{n} u_{n - k} v_{k}.
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>


<exercise>
    <title>Loi faible des grands nombres dans <m>L^1</m></title>
    <p> </p>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \geq 1}</m> une suite de variables aléatoires réelles discrètes, deux à deux indépendantes, de même loi, possédant une espérance finie <m>m</m>. On pose, pour tout <m>n \in \mathbb{N}^*</m>, <m>Y_n = \frac{X_1 + \cdots + X_n}{n}</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Dans les deux premières questions, on suppose <m>m = 0</m>.
                <ol>
                    <li>
                        <p>Soit <m>\varepsilon \gt 0</m>.
                        <ol>
                            <li>
                                Pour <m>c \gt 0</m>, on définit <m>g : \mathbb{R} \to \mathbb{R}</m> par :
                                <me>
                                    g(x) = 
                                    \begin{cases} 
                                        x \amp \text{si } |x| \leq c \\
                                        0 \amp \text{sinon}.
                                    \end{cases}
                                </me>
                                Montrer que la variable aléatoire <m>g(X_1)</m> est d'espérance finie et que l'on peut choisir <m>c</m> tel que <m>\mathbb{E}(|g(X_1) - X_1|) \leq \frac{\varepsilon}{2}</m>.
                            </li>
                            <li>
                                On pose <m>a = \mathbb{E}(g(X_1))</m>. Montrer que :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon.
                                </me>
                            </li>
                            <li>
                                On pose, pour tout <m>n \in \mathbb{N}^*</m>, <m>U_n = g(X_n) - a</m> et <m>Y_n' = \frac{U_1 + \cdots + U_n}{n}</m>. Justifier que les variables <m>U_n</m> admettent un moment d'ordre 2. Montrer que <m>\lim_{n \to +\infty} \mathbb{V}(Y_n') = 0</m>. En déduire que <m>\lim_{n \to +\infty} \mathbb{E}(|Y_n|) = 0</m>.
                            </li>
                        </ol></p>
                    </li>
                    <li>
                        Montrer que, pour tout <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \lim_{n \to +\infty} \mathbb{P}(|Y_n| \geq \varepsilon) = 0.
                        </me>
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        <ol>
                            <li>
                                La fonction <m>g</m> est bornée par <m>c</m>, donc <m>g(X_1)</m> est d'espérance finie. De plus, on a :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1|) = \mathbb{E}(|X_1| \mathbf{1}_{\{|X_1| \gt c\}}).
                                </me>
                                Comme <m>\mathbb{E}(|X_1|) \lt +\infty</m>, on peut choisir <m>c</m> suffisamment grand pour que <m>\mathbb{E}(|X_1| \mathbf{1}_{\{|X_1| \gt c\}}) \leq \frac{\varepsilon}{2}</m>.
                            </li>
                            <li>
                                On a :
                                <me>
                                    \mathbb{E}(|g(X_1) - X_1 - a|) \leq \mathbb{E}(|g(X_1) - X_1|) + |a|.
                                </me>
                                Comme <m>a = \mathbb{E}(g(X_1))</m> et <m>|a| \leq \mathbb{E}(|g(X_1)|) \leq c</m>, on peut choisir <m>c</m> suffisamment grand pour que <m>\mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon</m>.
                            </li>
                            <li>
                                Les variables <m>U_n</m> sont centrées et admettent un moment d'ordre 2 car <m>g(X_n)</m> est bornée et <m>X_n</m> a une espérance finie. On a :
                                <me>
                                    \mathbb{V}(Y_n') = \frac{\mathbb{V}(U_1)}{n} \to 0 \quad \text{quand} \quad n \to +\infty.
                                </me>
                                Par l'inégalité de Markov, on a :
                                <me>
                                    \mathbb{E}(|Y_n|) \leq \mathbb{E}(|Y_n'|) + \mathbb{E}(|Y_n - Y_n'|) \leq \sqrt{\mathbb{V}(Y_n')} + \mathbb{E}(|Y_n - Y_n'|).
                                </me>
                                Comme <m>\mathbb{E}(|Y_n - Y_n'|) \leq \mathbb{E}(|g(X_1) - X_1 - a|) \leq \varepsilon</m>, on obtient :
                                <me>
                                    \lim_{n \to +\infty} \mathbb{E}(|Y_n|) = 0.
                                </me>
                            </li>
                        </ol>
                    </li>
                    <li>
                        Pour tout <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \mathbb{P}(|Y_n| \geq \varepsilon) \leq \frac{\mathbb{E}(|Y_n|)}{\varepsilon} \to 0 \quad \text{quand} \quad n \to +\infty.
                        </me>
                    </li>
                </ol>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                On ne suppose plus <m>m = 0</m>. Montrer que l'on a :
                <me>
                    \forall \varepsilon \gt 0, \quad \lim_{n \to +\infty} \mathbb{P}(|Y_n - m| \geq \varepsilon) = 0.
                </me>
            </p>
        </statement>
        <solution>
            <p>
                On pose <m>Z_n = Y_n - m</m>. Alors <m>Z_n</m> est la moyenne des variables <m>X_n - m</m>, qui sont centrées. D'après la question précédente, on a :
                <me>
                    \lim_{n \to +\infty} \mathbb{P}(|Z_n| \geq \varepsilon) = 0.
                </me>
                Par conséquent :
                <me>
                    \lim_{n \to +\infty} \mathbb{P}(|Y_n - m| \geq \varepsilon) = 0.
                </me>
            </p>
        </solution>
    </task>
</exercise>


<exercise>
    <title>Modèle de Galton-Watson</title>
    <introduction>
        <p>
            On observe des virus qui se reproduisent tous selon la même loi avant de mourir : un virus donne naissance en une journée à <m>X</m> virus, où <m>X</m> est une variable aléatoire à valeurs dans <m>\N</m>. Pour tout <m>k \in \N</m>, on note <m>\Pr(X = k) = p_{k}</m>. On suppose <m>p_{1}\gt0</m> et <m>p_{0} + p_{1} \lt 1</m>. On note <m>f</m> la fonction génératrice de <m>X</m>. On part au jour zéro de <m>X_{0} = 1</m> virus. Au premier jour, on a donc <m>X_{1}</m> virus, où <m>X_{1}</m> suit la loi de <m>X</m> ; chacun de ces <m>X_{1}</m> virus évolue alors indépendamment des autres virus et se reproduit selon la même loi avant de mourir : cela conduit à avoir <m>X_{2}</m> virus au deuxième jour ; et le processus continue de la sorte. On note <m>u_{n} = \Pr(X_{n} = 0)</m>.
        </p>
        </introduction>
        <task>
            <p>
                Calculer <m>u_{0}</m>, <m>u_{1}</m>.
            </p>
        </task>
        <task>
            <p>
                Montrer que la suite <m>(u_{n})_{n \in \N}</m> est convergente.
            </p>
        </task>
        <task>
            <p>
                Montrer que pour tout entier <m>n \geqslant 0</m>, on a <m>u_{n+1} = f(u_{n})</m>.
            </p>
        </task>
        <task>
            <p>
                Que peut-on dire de la limite de <m>(u_{n})_{n \in \N}</m> ? Discuter selon la valeur de <m>\Es(X)</m>. Interpréter le résultat.
            </p>
        </task>
</exercise>


<exercise>
    <title>Somme aléatoire de variables aléatoires</title>
    <introduction>
        <p>
            Soit <m>(X_{n})_{n \geqslant 1}</m> une suite de variables aléatoires réelles discrètes, toutes de même loi, et <m>N</m> une variable aléatoire à valeurs dans <m>\N</m>. On suppose que <m>N</m> et les variables <m>X_{n}</m> pour <m>n \in \N^{*}</m> forment une suite de variables aléatoires indépendantes. On pose :
            <me>
                \forall n \in \N^{*} \quad S_{n} = \sum_{k=1}^{n} X_{k} \quad \text{et} \quad S_{0} = 0.
            </me>
        </p>
        </introduction>
        <task>
            <p>
                Montrer que <m>S_{N}</m> est une variable aléatoire.
            </p>
        </task>
        <task>
            <p>
                Déterminer la loi de <m>S_{N}</m> lorsque les <m>X_{k}</m> suivent la loi de Bernoulli de paramètre <m>p</m> et <m>N</m> la loi de Poisson de paramètre <m>\lambda</m>.
            </p>
        </task>
        <task>
            <p>
                Déterminer la loi de <m>S_{N}</m> lorsque les <m>X_{k}</m> suivent la loi géométrique de paramètre <m>p</m> et <m>N</m> la loi géométrique de paramètre <m>p^{\prime}</m>.
            </p>
        </task>
        <task>
            <p>
                On suppose que les variables aléatoires <m>X_{n}</m> sont à valeurs dans <m>\N</m>.
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que <m>G_{S_{N}} = G_{N} \circ G_{X_{1}}</m> sur <m>[0, 1]</m>.
                    </p>
                </li>
                <li>
                    <p>
                        Montrer que, si <m>X_{1}</m> et <m>N</m> sont d'espérance finie, alors <m>S_{N}</m> est d'espérance finie et vérifie la première formule de Wald :
                        <me>
                            \Es(S_{N}) = \Es(X_{1}) \Es(N).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        Montrer que, si <m>X_{1}</m> et <m>N</m> possèdent un moment d'ordre 2, alors <m>S_{N}</m> possède aussi un moment d'ordre 2 et vérifie la seconde formule de Wald :
                        <me>
                            \Va(S_{N}) = \Va(X_{1}) \Es(N) + (\Es(X_{1}))^{2} \Va(N).
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>


<exercise>
    <title>Succès consécutifs</title>
    <introduction>
        <p>
            On considère une suite d'épreuves de Bernoulli indépendantes. À chaque épreuve, la probabilité de succès est <m>p \in ]0, 1[</m>. On se donne un entier <m>r</m> strictement positif. Pour <m>n \in \N^{*}</m>, on note <m>\Pi_{n}</m> la probabilité qu'au cours des <m>n</m> premières épreuves, on ait obtenu <m>r</m> succès consécutifs (au moins une fois).
        </p>
        </introduction>
        <task>
            <p>
                Calculer <m>\Pi_{0}</m>, <m>\Pi_{1}</m>, ..., <m>\Pi_{r}</m>.
            </p>
        </task>
        <task>
            <p>
                Montrer que, pour <m>n \geqslant r</m>, on a <m>\Pi_{n+1} = \Pi_{n} + (1 - \Pi_{n-r}) p^{r} (1 - p)</m>.
            </p>
        </task>
        <task>
            <p>
                Montrer que la suite <m>(\Pi_{n})_{n \in \N}</m> est convergente. Calculer sa limite.
            </p>
        </task>
        <task>
            <p>
                Déduire de la question 1 que l'on peut définir une variable aléatoire <m>T</m> égale au temps d'attente de <m>r</m> succès consécutifs. On définira <m>(T = k)</m> comme l'événement « on a obtenu des succès aux épreuves de rang <m>k - r + 1</m>, <m>k - r + 2</m>, ..., <m>k</m> sans jamais avoir obtenu <m>r</m> succès consécutifs auparavant ».
            </p>
        </task>
        <task>
            <p>
                Montrer que :
                <me>
                    \Es(T) = \frac{1 - p^{r}}{(1 - p) p^{r}}.
                </me>
            </p>
        </task>
</exercise>


<exercise>
    <title>Marche aléatoire dans <m>\mathbb{Z}</m> : premier retour à l'origine</title>
    <introduction>
        <p>
            Soit <m>(X_{n})_{n \in \N^{*}}</m> une suite de variables aléatoires, sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m>, indépendantes et de même loi définie par :
            <me>
                \Pr(X_{n} = 1) = p \quad \text{et} \quad \Pr(X_{n} = -1) = 1 - p,
            </me>
            où <m>p \in [0, 1]</m>. On pose <m>S_{0} = 0</m> et, pour tout <m>n \in \N^{*}</m>, <m>S_{n} = \sum_{k=1}^{n} X_{k}</m>. La suite <m>(S_{n})</m> est appelée marche aléatoire dans <m>\mathbb{Z}</m>.
        </p>
    </introduction>
        <task>
            <p>
                Déterminer <m>u_{n} = \Pr(S_{n} = 0)</m> pour tout <m>n \in \N</m>.
            </p>
        </task>
        <task>
            <p>
                On note <m>f(x)</m> la somme de la série entière <m>\sum u_{n} x^{n}</m>. Montrer que :
                <me>
                    \forall x \in ]-1, 1[ \quad f(x) = \frac{1}{\sqrt{1 - 4 p (1 - p) x^{2}}}.
                </me>
            </p>
        </task>
        <task>
            <p>
                Pour tout entier naturel non nul <m>k</m>, on note <m>A_{k}</m> l'événement « le mobile retourne pour la première fois à l'origine au bout de <m>k</m> déplacements », c'est-à-dire :
                <me>
                    A_{k} = (S_{k} = 0) \cap \left(\bigcap_{i=1}^{k-1} (S_{i} \neq 0)\right).
                </me>
                On pose <m>v_{k} = \Pr(A_{k})</m> pour tout <m>k \geqslant 1</m> et <m>v_{0} = 0</m>.
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que, pour tout entier naturel <m>n</m> non nul, on a :
                        <me>
                            (S_{n} = 0) = \sum_{k=1}^{n} \Pr((S_{n} = 0) \cap A_{k}).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire que, pour tout entier naturel non nul <m>n</m>, on a :
                        <me>
                            u_{n} = \sum_{k=0}^{n} u_{n - k} v_{k}.
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>



<exercise>
    <title>Inégalité de Kolmogorov</title>
    <introduction>
        <p>
            Soit <m>X_{1}, \ldots, X_{n}</m> des variables aléatoires réelles discrètes de l'espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m>, indépendantes, ayant un moment d'ordre 2, centrées, ainsi que <m>a \in \R_{+}^{*}</m>. On pose, pour tout <m>i \in \llbracket 1, n \rrbracket</m> :
            <me>
                S_{i} = X_{1} + \cdots + X_{i}, \quad B_{i} = \left\{\left|S_{1}\right| \lt a\right\} \cap \ldots \cap \left\{\left|S_{i-1}\right| \lt a\right\} \cap \left\{\left|S_{i}\right| \geqslant a\right\}.
            </me>
        </p>
    </introduction>
        <task>
            <p>
                Montrer que, pour <m>i \in \llbracket 1, n \rrbracket</m>, les variables <m>S_{i} \mathbf{1}_{B_{i}}</m> et <m>S_{n} - S_{i}</m> sont indépendantes. En déduire que :
                <me>
                    \Es(S_{n}^{2} \mathbf{1}_{B_{i}}) = \Es(S_{i}^{2} \mathbf{1}_{B_{i}}) + \Es((S_{n} - S_{i})^{2} \mathbf{1}_{B_{i}}) \geqslant a^{2} \Pr(B_{i}).
                </me>
            </p>
        </task>
        <task>
            <p>
                On pose <m>C = \left\{\sup \left(\left|S_{1}\right|, \left|S_{2}\right|, \ldots, \left|S_{n}\right|\right) \geqslant a\right\}</m>. Montrer que <m>\Pr(C) = \sum_{i=1}^{n} \Pr(B_{i})</m>.
            </p>
        </task>
        <task>
            <p>
                En déduire l'inégalité de Kolmogorov :
                <me>
                    \Pr\left(\sup \left(\left|S_{1}\right|, \left|S_{2}\right|, \ldots, \left|S_{n}\right|\right) \geqslant a\right) \leqslant \frac{\Va(S_{n})}{a^{2}}.
                </me>
            </p>
        </task>
</exercise>



<exercise>
    <title>Inégalité de Le Cam</title>
    <introduction>
        <p>
            L'objet de l'exercice est d'étudier l'approximation de la loi binomiale par la loi de Poisson. Toutes les variables aléatoires considérées sont définies sur le même espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m> et sont à valeurs dans <m>\N</m>.
        </p>
    </introduction>
        <task>
            <p>
                Soit <m>X</m> et <m>Y</m> deux telles variables aléatoires. Pour tout <m>k \in \N</m>, on pose <m>p_{k} = \Pr(X = k)</m> et <m>q_{k} = \Pr(Y = k)</m>. On définit la distance entre <m>X</m> et <m>Y</m> par :
                <me>
                    \mathrm{d}(X, Y) = \frac{1}{2} \sum_{k=0}^{+\infty} \left|p_{k} - q_{k}\right|.
                </me>
            </p>
            <ol marker="1.">
                <li>
                    <p>
                        Montrer que pour toute partie <m>A</m> de <m>\N</m>, on a :
                        <me>
                            |\Pr(X \in A) - \Pr(Y \in A)| \leqslant \mathrm{d}(X, Y).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        Démontrer la formule :
                        <me>
                            \mathrm{d}(X, Y) = 1 - \sum_{k=0}^{+\infty} \min(p_{k}, q_{k}).
                        </me>
                    </p>
                </li>
                <li>
                    <p>
                        En déduire :
                        <me>
                            \mathrm{d}(X, Y) \leqslant \Pr(X \neq Y).
                        </me>
                    </p>
                </li>
            </ol>
        </task>
</exercise>



<exercise>
    <title>Convergence presque sûre</title>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \in \mathbb{N}}</m> une suite de variables aléatoires réelles et <m>X</m> une variable aléatoire réelle définies sur <m>(\Omega, \mathcal{A}, \mathbb{P})</m>. On pose :
            <me>
                B = \left\{ \omega \in \Omega \mid \lim_{n \to +\infty} X_n(\omega) = X(\omega) \right\}.
            </me>
            On dit que la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m> si <m>\mathbb{P}(B) = 1</m>.
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que l'on a <m>\mathbb{P}(B) = \lim_{k \to +\infty} \mathbb{P}(C_k)</m>, où :
                <me>
                    C_k = \bigcap_{n \in \mathbb{N}} \bigcup_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\}.
                </me>
            </p>
        </statement>
        <solution>
            <p>
                On a :
                <me>
                    B = \left\{ \omega \in \Omega \mid \forall k \in \mathbb{N}^*, \exists n \in \mathbb{N}, \forall p \geq n, |X_p(\omega) - X(\omega)| \leq \frac{1}{k} \right\}.
                </me>
                En d'autres termes, <m>B</m> est l'ensemble des <m>\omega \in \Omega</m> pour lesquels la suite <m>(X_n(\omega))</m> converge vers <m>X(\omega)</m>. On peut réécrire <m>B</m> comme :
                <me>
                    B = \bigcap_{k \in \mathbb{N}^*} \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\}.
                </me>
                Par continuité décroissante de la probabilité, on a :
                <me>
                    \mathbb{P}(B) = \lim_{k \to +\infty} \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \leq \frac{1}{k} \right\} \right) = \lim_{k \to +\infty} \mathbb{P}(C_k).
                </me>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                On suppose que :
                <me>
                    \forall \varepsilon \gt 0, \quad \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Montrer que la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </statement>
        <solution>
            <p>
                Par hypothèse, pour tout <m>\varepsilon \gt 0</m>, on a :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Cela signifie que, pour tout <m>\varepsilon \gt 0</m>, l'ensemble des <m>\omega \in \Omega</m> pour lesquels <m>|X_p(\omega) - X(\omega)| \gt \varepsilon</m> pour une infinité de <m>p</m> est de probabilité nulle. Par conséquent, pour presque tout <m>\omega \in \Omega</m>, la suite <m>(X_n(\omega))</m> converge vers <m>X(\omega)</m>, c'est-à-dire que <m>\mathbb{P}(B) = 1</m>.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Montrer que si la série de terme général <m>\mathbb{P}(|X_n - X| \gt \varepsilon)</m> converge pour tout <m>\varepsilon \gt 0</m>, alors la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </statement>
        <solution>
            <p>
                Si la série de terme général <m>\mathbb{P}(|X_n - X| \gt \varepsilon)</m> converge pour tout <m>\varepsilon \gt 0</m>, alors, d'après le lemme de Borel-Cantelli, on a :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                En effet, le lemme de Borel-Cantelli affirme que si <m>\sum_{n=1}^{+\infty} \mathbb{P}(A_n) \lt +\infty</m>, alors <m>\mathbb{P}\left( \limsup_{n \to +\infty} A_n \right) = 0</m>. Appliqué à <m>A_n = \{ |X_n - X| \gt \varepsilon \}</m>, cela donne :
                <me>
                    \mathbb{P}\left( \bigcup_{n \in \mathbb{N}} \bigcap_{p \geq n} \left\{ |X_p - X| \gt \varepsilon \right\} \right) = 0.
                </me>
                Par conséquent, la suite <m>(X_n)_{n \in \mathbb{N}}</m> converge presque sûrement vers <m>X</m>.
            </p>
        </solution>
    </task>
</exercise>


<exercise>
    <title>Fonction génératrice des moments</title>
    <introduction>
        <p>
            Soit <m>X</m> une variable aléatoire discrète, pas presque sûrement constante, sur l'espace probabilisé <m>(\Omega, \mathcal{A}, \Pr)</m>. On pose, pour <m>t \in \R</m>, <m>L_{X}(t) = \Es(e^{t X})</m> (la fonction <m>L_{X}</m> est appelée fonction génératrice des moments de la variable aléatoire <m>X</m>). On suppose qu'il existe un intervalle <m>]\alpha, \beta[</m> contenant 0 tel que <m>L_{X}(t) \lt +\infty</m> pour tout <m>t \in ]\alpha, \beta[</m>.
        </p>
    </introduction>
        <task>
            <ol>
            <li><p>
                Soit <m>a \lt b</m> deux réels tels que <m>[a, b] \subset ]\alpha, \beta[</m>. On considère <m>\delta\gt0</m> tel que <m>[a - \delta, b + \delta] \subset ]\alpha, \beta[</m>. Soit <m>k \in \N</m>. Montrer qu'il existe <m>C\gt0</m> tel que :
                <me>
                    \forall t \in [a, b] \quad \forall u \in \R \quad |u|^{k} e^{t u} \leqslant C \left(e^{(a - \delta) u} + e^{(b + \delta) u}\right).
                </me>
                En déduire que <m>X^{k} e^{t X}</m> est d'espérance finie pour tout <m>t \in ]\alpha, \beta[</m>.
            </p>
            </li>
            <li>
            <p>
                Montrer que <m>L_{X}</m> est de classe <m>\mathcal{C}^{\infty}</m> sur <m>]\alpha, \beta[</m> et vérifie :
                <me>
                    \forall t \in ]\alpha, \beta[ \quad \forall k \in \N \quad L_{X}^{(k)}(t) = \Es(X^{k} e^{t X}).
                </me>
                En déduire, pour tout <m>k \in \N</m>, une expression du moment d'ordre <m>k</m> de <m>X</m>. On note <m>m</m> l'espérance de <m>X</m>.
            </p>
        </li>
        </ol>
        </task>

    <task>
        <statement>
            <p>
                Montrer que la fonction <m>\Psi_X(t) = \ln L_X(t)</m> est strictement convexe.
            </p>
        </statement>
        <solution>
            <p>
                La fonction <m>\Psi_X(t) = \ln L_X(t)</m> est strictement convexe car sa dérivée seconde est strictement positive. En effet, on a :
                <me>
                    \Psi_X''(t) = \frac{L_X''(t) L_X(t) - (L_X'(t))^2}{L_X(t)^2} \gt 0,
                </me>
                car <m>L_X''(t) L_X(t) - (L_X'(t))^2 = \mathbb{E}(X^2 e^{tX}) \mathbb{E}(e^{tX}) - (\mathbb{E}(X e^{tX}))^2 \gt 0</m> par l'inégalité de Cauchy-Schwarz.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Montrer que les inégalités de Chernov suivantes sont vérifiées :
                <me>
                    \mathbb{P}(X \leq c) \leq e^{-g(c)} \quad \text{si} \quad c \lt m,
                </me>
                <me>
                    \mathbb{P}(X \geq c) \leq e^{-g(c)} \quad \text{si} \quad c \gt m,
                </me>
                où <m>g(c) = \sup_{t \in ]\alpha, \beta[} (ct - \Psi_X(t))</m>.
            </p>
        </statement>
        <solution>
            <p>
                Les inégalités de Chernov sont obtenues en utilisant la convexité de <m>\Psi_X</m>. Pour <m>c \lt m</m>, on a :
                <me>
                    \mathbb{P}(X \leq c) = \mathbb{P}(e^{tX} \leq e^{tc}) \leq \frac{\mathbb{E}(e^{tX})}{e^{tc}} = e^{-(ct - \Psi_X(t))}.
                </me>
                En prenant le supremum sur <m>t \in ]\alpha, \beta[</m>, on obtient :
                <me>
                    \mathbb{P}(X \leq c) \leq e^{-g(c)}.
                </me>
                De même, pour <m>c \gt m</m>, on a :
                <me>
                    \mathbb{P}(X \geq c) = \mathbb{P}(e^{tX} \geq e^{tc}) \leq \frac{\mathbb{E}(e^{tX})}{e^{tc}} = e^{-(ct - \Psi_X(t))}.
                </me>
                En prenant le supremum sur <m>t \in ]\alpha, \beta[</m>, on obtient :
                <me>
                    \mathbb{P}(X \geq c) \leq e^{-g(c)}.
                </me>
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                On considère une suite <m>(X_n)_{n \geq 1}</m> de variables aléatoires indépendantes suivant la même loi que <m>X</m>. On pose, pour tout <m>n \in \mathbb{N}^*</m>, <m>S_n = \sum_{k=1}^n X_k</m>.
                <ol>
                    <li>
                        Montrer que, pour tout <m>c \in \mathbb{R}</m>, on a :
                        <me>
                            \mathbb{P}\left(\frac{S_n}{n} \leq c\right) \leq e^{-n g(c)} \quad \text{si} \quad c \lt m,
                        </me>
                        <me>
                            \mathbb{P}\left(\frac{S_n}{n} \geq c\right) \leq e^{-n g(c)} \quad \text{si} \quad c \gt m.
                        </me>
                    </li>
                    <li>
                        Montrer que, pour tout <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \mathbb{P}\left(\left|\frac{S_n}{n} - m\right| \geq \varepsilon\right) \leq 2 e^{-n \min(g(m + \varepsilon), g(m - \varepsilon))}.
                        </me>
                    </li>
                    <li>
                        En utilisant le résultat de l'exercice 16.25, montrer que la suite <m>\left(\frac{S_n}{n}\right)</m> converge presque sûrement vers <m>m</m>.
                    </li>
                </ol>
            </p>
        </statement>
        <solution>
            <p>
                <ol>
                    <li>
                        Pour <m>c \lt m</m>, on a :
                        <me>
                            \mathbb{P}\left(\frac{S_n}{n} \leq c\right) = \mathbb{P}\left(S_n \leq n c\right) \leq e^{-n g(c)},
                        </me>
                        où <m>g(c) = \sup_{t \in ]\alpha, \beta[} (ct - \Psi_X(t))</m>. De même, pour <m>c \gt m</m>, on a :
                        <me>
                            \mathbb{P}\left(\frac{S_n}{n} \geq c\right) = \mathbb{P}\left(S_n \geq n c\right) \leq e^{-n g(c)}.
                        </me>
                    </li>
                    <li>
                        Pour <m>\varepsilon \gt 0</m>, on a :
                        <me>
                            \mathbb{P}\left(\left|\frac{S_n}{n} - m\right| \geq \varepsilon\right) = \mathbb{P}\left(\frac{S_n}{n} \leq m - \varepsilon\right) + \mathbb{P}\left(\frac{S_n}{n} \geq m + \varepsilon\right).
                        </me>
                        En appliquant les inégalités de Chernov, on obtient :
                        <me>
                            \mathbb{P}\left(\left|\frac{S_n}{n} - m\right| \geq \varepsilon\right) \leq e^{-n g(m - \varepsilon)} + e^{-n g(m + \varepsilon)} \leq 2 e^{-n \min(g(m + \varepsilon), g(m - \varepsilon))}.
                        </me>
                    </li>
                    <li>
                        D'après l'exercice 16.25, si la série de terme général <m>\mathbb{P}(|X_n - X| \gt \varepsilon)</m> converge pour tout <m>\varepsilon \gt 0</m>, alors la suite <m>(X_n)</m> converge presque sûrement vers <m>X</m>. En appliquant ce résultat à <m>\left(\frac{S_n}{n}\right)</m>, on obtient que <m>\left(\frac{S_n}{n}\right)</m> converge presque sûrement vers <m>m</m>.
                    </li>
                </ol>
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <title>Théorème de Weierstrass</title>
    <introduction>
        <p>
            Soit <m>f</m> une fonction continue de <m>[0, 1]</m> dans <m>\R</m>. Soit <m>x \in [0, 1]</m>. On considère une suite <m>(X_{n})_{n \geqslant 1}</m> de variables de Bernoulli de paramètre <m>x</m>, indépendantes, sur le même espace probabilisé. Pour <m>n \geqslant 1</m>, on pose <m>Y_{n} = \frac1n(X_{1} + \cdots + X_{n})</m>.
        </p>
        </introduction>
        <!-- <task>
            <p>
                Soit <m>\varepsilon\gt0</m>. Par uniforme continuité de <m>f</m> sur <m>[0, 1]</m>, il existe <m>\eta\gt0</m> tel que :
                <me>
                    \forall(t, u) \in [0, 1]^{2} \quad |t - u| \leqslant \eta \Longrightarrow |f(t) - f(u)| \leqslant \varepsilon.
                </me>
            </p>
        </task> -->
        <task>
            <p>
                Montrer que :
                <me>
                    \forall(t, u) \in [0, 1]^{2} \quad |f(t) - f(u)| \leqslant \frac{2 \Vert f \Vert _{\infty} (t - u)^{2}}{\eta^{2}} + \varepsilon.
                </me>
            </p>
        </task>
        <task>
            <p>
                Montrer que :
                <me>
                    \left|\Es(f(Y_{n})) - f(x)\right| \leqslant \frac{2 \Vert f \Vert _{\infty} \Va(Y_{n})}{\eta^{2}} + \varepsilon \leqslant \frac{2 \Vert f \Vert _{\infty}}{n \eta^{2}} + \varepsilon.
                </me>
            </p>
        </task>
        <task >
            <statement>
                <p>
                   En déduire que la suite de fonctions polynomiales <m>(B_n(f))_n</m> définie par 
                   <me>
                    \forall t\in[0,1],\;
                    B_n(f)(t)\sum_{k=0}^nf(k/n)\binom{n}{k}t^k(1-t)^{n-k}
                   </me>
                   converge uniformément vers <m>f</m> sur <m>[0,1]</m>.
                </p>
            </statement>
        </task>
</exercise>

<exercise>
    <title>File d'attente</title>
    <introduction>
        <p>
            Soit <m>n</m> un entier supérieur ou égal à 2. On considère une file d'attente avec un guichet et <m>n</m> clients qui attendent. Chaque minute, un guichet se libère. Le guichetier choisit alors le client qu'il appelle selon le processus aléatoire suivant :
            <ul>
                <li>avec probabilité 2, il appelle le client en première position dans la file,</li>
                <li>sinon, il choisit de manière équiprobable parmi les <m>n - 1</m> autres clients.</li>
            </ul>
            Enfin, un nouveau client arrive dans la file et se place en dernière position (de telle sorte qu'il y a toujours exactement <m>n</m> clients qui attendent). Pour tout <m>k \in \llbracket 1, n \rrbracket</m>, on note <m>T_{k}</m> le temps d'attente d'un client qui se trouve en position <m>k</m> dans la file.
        </p>
        </introduction>
        <task>
            <p>
                Quelle est la loi de <m>T_{1}</m> ? Donner son espérance, sa variance.
            </p>
        </task>
        <task>
            <p>
                Montrer que, pour tout <m>k \in \llbracket 1, n \rrbracket</m>, la variable <m>T_{k}</m> est d'espérance finie.
            </p>
        </task>
        <task>
            <p>
                Écrire une relation entre <m>\Es(T_{k})</m> et <m>\Es(T_{k-1})</m> pour tout <m>k \geqslant 2</m>. En déduire une expression de <m>\Es(T_{k})</m> en fonction de <m>k</m> et <m>n</m>. On pourra considérer la suite <m>((n + k - 2) \Es(T_{k}))_{1 \leqslant k \leqslant n}</m>.
            </p>
        </task>
        <task>
            <p>
                Comparer les caractéristiques de cette file d'attente et d'une file d'attente « classique » (premier arrivé, premier servi).
            </p>
        </task>
</exercise>

<exercise>
    <introduction>
        <p>
            Soit <m>(X_n)_{n \geq 1}</m> une suite de variables aléatoires réelles discrètes, toutes de même loi, et <m>N</m> une variable aléatoire à valeurs dans <m>\mathbb{N}</m>. On suppose que <m>N</m> et les variables <m>X_n</m>, pour <m>n \in \mathbb{N}^*</m>, forment une suite de variables aléatoires indépendantes. On pose :
            <me>
                \forall n \in \mathbb{N}^*, \quad S_n = \sum_{k=1}^n X_k \quad \text{et} \quad S_0 = 0.
            </me>
        </p>
    </introduction>

    <task>
        <statement>
            <p>
                Montrer que <m>S_N</m> est une variable aléatoire.
            </p>
        </statement>
        <solution>
            <p>
                On a <m>S_N(\Omega) \subset \{0\} \cup \bigcup_{n \in \mathbb{N}^*} S_n(\Omega)</m>. Les <m>S_n(\Omega)</m> étant au plus dénombrables, il en est de même de leur union dénombrable, et a fortiori <m>S_N(\Omega)</m> est au plus dénombrable. De plus :
                <me>
                    \forall x \in S_N(\Omega), \quad \{S_N = x\} = \bigcup_{n \in \mathbb{N}} \{S_n = x\} \cap \{N = n\},
                </me>
                donc <m>\{S_N = x\}</m>, union dénombrable d'événements, est un événement et <m>S_N</m> est une variable aléatoire.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>S_N</m>, lorsque les <m>X_k</m> suivent la loi de Bernoulli de paramètre <m>p</m> et <m>N</m> la loi de Poisson de paramètre <m>\lambda</m>.
            </p>
        </statement>
        <solution>
            <p>
                On a <m>S_N(\Omega) = \mathbb{N}</m> et d'après la formule des probabilités totales, pour tout <m>k \in \mathbb{N}</m> :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=0}^{+\infty} \mathbb{P}(S_N = k \mid N = n) \mathbb{P}(N = n).
                </me>
                On remarque que :
                <me>
                    \mathbb{P}(S_N = k \mid N = n) = \mathbb{P}(S_n = k \mid N = n) = \mathbb{P}(S_n = k),
                </me>
                la dernière égalité résultant de l'indépendance des variables <m>X_n</m> par rapport à <m>N</m>. Pour <m>n \geq 1</m>, <m>S_n</m> est une somme de <m>n</m> variables de Bernoulli, indépendantes, de même paramètre <m>p</m> ; elle suit donc la loi binomiale de paramètre <m>(n, p)</m>. On a donc :
                <me>
                    \mathbb{P}(S_n = k) = 
                    \begin{cases} 
                        \binom{n}{k} p^k (1 - p)^{n - k}  \amp   \text{si } k \leq n \\
                        0  \amp   \text{si } k \gt n.
                    \end{cases}
                </me>
                On remarque que cette formule reste vérifiée si <m>n = 0</m>, car alors <m>S_n</m> prend la valeur 0, avec la probabilité 1. On en déduit :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=k}^{+\infty} \binom{n}{k} p^k (1 - p)^{n - k} e^{-\lambda} \frac{\lambda^n}{n!}.
                </me>
                En simplifiant, on obtient :
                <me>
                    \mathbb{P}(S_N = k) = e^{-\lambda p} \frac{(\lambda p)^k}{k!}.
                </me>
                La variable aléatoire <m>S_N</m> suit donc la loi de Poisson de paramètre <m>\lambda p</m>.
            </p>
        </solution>
    </task>

    <task>
        <statement>
            <p>
                Déterminer la loi de <m>S_N</m> lorsque les <m>X_k</m> suivent la loi géométrique de paramètre <m>p</m> et <m>N</m> la loi géométrique de paramètre <m>p'</m>.
            </p>
        </statement>
        <solution>
            <p>
                On procède comme dans la question précédente. On a <m>S_N(\Omega) = \mathbb{N}^*</m> et, pour tout <m>n \in \mathbb{N}^*</m> :
                <me>
                    \mathbb{P}(S_N = k \mid N = n) = \mathbb{P}(S_n = k \mid N = n) = \mathbb{P}(S_n = k).
                </me>
                Comme les variables <m>X_k</m> sont à valeurs dans <m>\mathbb{N}^*</m>, il est clair que <m>\mathbb{P}(S_n = k) = 0</m> si <m>k \lt  n</m>. Supposons <m>k \geq n</m>. On peut alors écrire :
                <me>
                    \{S_n = k\} = \bigcup_{(i_1, i_2, \ldots, i_n) \in J_k} \{X_1 = i_1\} \cap \{X_2 = i_2\} \cap \ldots \cap \{X_n = i_n\},
                </me>
                où <m>J_k</m> est l'ensemble des <m>n</m>-listes d'entiers strictement positifs <m>(i_1, i_2, \ldots, i_n)</m> tels que <m>i_1 + i_2 + \cdots + i_n = k</m>. Le cardinal de <m>J_k</m> est égal au nombre de <m>(n-1)</m>-listes <m>(j_1, j_2, \ldots, j_{n-1})</m> d'entiers tels que <m>1 \leq j_1 \lt  j_2 \lt  \ldots \lt  j_{n-1} \leq k - 1</m>, car à <m>(i_1, i_2, \ldots, i_n)</m>, on peut associer bijectivement :
                <me>
                    (j_1, j_2, \ldots, j_{n-1}) = (i_1, i_1 + i_2, \ldots, i_1 + i_2 + \cdots + i_{n-1}).
                </me>
                On a donc <m>\text{card}(J_k) = \binom{k-1}{n-1}</m>.
                D'autre part, pour <m>(i_1, i_2, \ldots, i_n) \in J_k</m>, on a, par indépendance des variables aléatoires <m>X_i</m> :
                <me>
                    \mathbb{P}(\{X_1 = i_1\} \cap \{X_2 = i_2\} \cap \ldots \cap \{X_n = i_n\}) = \prod_{j=1}^n \mathbb{P}(X_j = i_j) = \prod_{j=1}^n p(1 - p)^{i_j - 1} = p^n (1 - p)^{k - n}.
                </me>
                On obtient ainsi <m>\mathbb{P}(S_n = k) = \binom{k-1}{n-1} p^n (1 - p)^{k - n}</m>. On en déduit, pour <m>k \in \mathbb{N}^*</m> :
                <me>
                    \mathbb{P}(S_N = k) = \sum_{n=1}^{+\infty} \mathbb{P}(S_N = k \mid N = n) \mathbb{P}(N = n) = \sum_{n=1}^k \binom{k-1}{n-1} p^n (1 - p)^{k - n} p' (1 - p')^{n - 1}.
                </me>
                En simplifiant, on obtient :
                <me>
                    \mathbb{P}(S_N = k) = p p' (1 - p p')^{k - 1}.
                </me>
                La variable <m>S_N</m> suit donc la loi géométrique de paramètre <m>p p'</m>.
            </p>
        </solution>
    </task>
</exercise>

<exercise>
    <introduction>
    <p>
        On considère une suite d'épreuves de Bernoulli indépendantes. À chaque épreuve, la probabilité de succès est <m>p \in ]0,1[</m>. On se donne un entier <m>r</m> strictement positif. Pour <m>n \in \mathbb{N}^*</m>, on note <m>\Pi_n</m> la probabilité qu'au cours des <m>n</m> premières épreuves, on ait obtenu <m>r</m> succès consécutifs (au moins une fois).
        </p>
    </introduction>

    <task>
        <statement>
            <ol>
                <li>Calculer <m>\Pi_0</m>, <m>\Pi_1</m>, ..., <m>\Pi_r</m>.</li>
                <li>Montrer que, pour <m>n \geq r</m>, on a <m>\Pi_{n+1} = \Pi_n + (1 - \Pi_{n-r}) p^r (1 - p)</m>.</li>
                <li>Montrer que la suite <m>(\Pi_n)_{n \in \mathbb{N}}</m> est convergente. Calculer sa limite.</li>
            </ol>
        </statement>
        <solution>
            <p>
            <ol>
                <li>
                    On a <m>\Pi_0 = \Pi_1 = \ldots = \Pi_{r-1} = 0</m> et <m>\Pi_r = p^r</m>.
                </li>
                <li>
                    Pour <m>n \geq 1</m>, on note <m>S_n</m> l'événement « la <m>n</m>-ième épreuve est un succès » et <m>A_n</m> l'événement « au cours de <m>n</m> premiers tirages, on a obtenus <m>r</m> succès consécutifs ». On a clairement <m>A_n \subset A_{n+1}</m> et donc <m>\Pi_{n+1} - \Pi_n = \mathbb{P}(A_{n+1} \setminus A_n)</m>. L'événement <m>A_{n+1} \setminus A_n</m> est réalisé si on obtient <m>r</m> succès consécutifs pour la première fois entre le <m>(n-r+2)</m>-ième et le <m>(n+1)</m>-ième tirage, ce qui impose que la <m>(n-r+1)</m>-ième épreuve soit un échec et qu'on n'ait pas avant obtenu <m>r</m> succès consécutifs. On a donc :
                    <me>
                        A_{n+1} \setminus A_n = A_{n-r} \cap S_{n-r+1} \cap S_{n-r+2} \cap \cdots \cap S_{n+1}
                    </me>
                    et par indépendance des épreuves :
                    <me>
                        \Pi_{n+1} - \Pi_n = (1 - \Pi_{n-r}) (1 - p) p^r.
                    </me>
                </li>
                <li>
                    La suite <m>(\Pi_n)_n</m> est croissante et majorée par 1 donc convergente. On note <m>L</m> sa limite. Par passage à la limite dans la relation précédente, on obtient <m>L - L = (1 - L)(1 - p)p^r</m>, et donc <m>L = 1</m>, puisque <m>p \in [0,1]</m>.
                </li>
            </ol>
        </p>
        </solution>

    </task>

    <task>
        <statement>
            <ol>
                <li>Déduire de la question 1 que l'on peut définir une variable aléatoire <m>T</m> égale au temps d'attente de <m>r</m> succès consécutifs. On définira <m>\{T = k\}</m> comme l'événement « on a obtenu des succès aux épreuves de rang <m>k-r+1</m>, <m>k-r+2</m>, ..., <m>k</m> sans jamais avoir obtenu <m>r</m> succès consécutifs auparavant ».</li>
                <li>Montrer en utilisant le résultat de l'exercice 18 de la page 911 que :
                    <me>
                        \mathbb{E}(T) = \frac{1 - p^r}{(1 - p)p^r}.
                    </me>
                </li>
            </ol>
        </statement>
        <solution>
            <ol>
                <li>
                    Comme la suite <m>(A_n)</m> est croissante, on a :
                    <me>
                        \mathbb{P} \left( \bigcup_{n \in \mathbb{N}^*} A_n \right) = \lim_{n \to +\infty} \Pi_n = 1.
                    </me>
                    On obtient de manière presque sûre une suite de <m>r</m> succès consécutifs au bout d'un nombre fini d'épreuves. Sur un ensemble de probabilité 1, on peut définir l'application <m>T</m>.
                    On a, par définition <m>\{T = k\} = A_{k+1} \setminus A_k</m>, donc <m>\{T = k\}</m> est un événement et <m>T</m> une variable aléatoire à valeurs dans <m>\mathbb{N}^*</m>.
                </li>
                <li>
                    Pour tout <m>k \in \mathbb{N}</m>, on a <m>\{T \gt k\} = A_k</m> et donc <m>\mathbb{P}(T \gt k) = 1 - \Pi_k</m>. D'après la question 1, on a pour tout <m>k \geq 0</m> :
                    <me>
                        1 - \Pi_k = \frac{\Pi_{k+r+1} - \Pi_{k+r}}{(1 - p)p^r}.
                    </me>
                    D'après l'exercice 18 de la page 911, on en déduit :
                    <me>
                        \mathbb{E}(T) = \sum_{k=0}^{+\infty} \frac{\Pi_{k+r+1} - \Pi_{k+r}}{(1 - p)p^r} = \frac{\lim_{n \to +\infty} \Pi_n - \Pi_r}{(1 - p)p^r} = \frac{1 - p^r}{(1 - p)p^r}.
                    </me>
                </li>
            </ol>
        </solution>
    </task>
</exercise>




 </chapter>
